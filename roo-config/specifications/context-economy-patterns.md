# üí∞ Patterns d'√âconomie Contexte - Optimisation Tokens

**Version :** 2.0.0 ‚ö†Ô∏è **R√âVISION MAJEURE FB-04**
**Date :** 2 Octobre 2025
**Architecture :** 2-Niveaux (Simple/Complex)
**Objectif :** Maximiser efficacit√© utilisation tokens ET qualit√© code
**R√©vision :** Principe Anti-Angles-Morts + D√©l√©gation Prioritaire

### üîó Lien avec Mapping LLMs

Les patterns d'√©conomie contexte sont **adapt√©s par tier LLM** :

- **[`llm-modes-mapping.md`](llm-modes-mapping.md)** : D√©finit l'optimisation budget par tier
  - **Modes Simples (Flash/Mini)** : Grounding minimal, d√©l√©gation prioritaire
  - **Modes Complex (SOTA)** : Grounding complet 4-niveaux SDDD
  - **Seuils tokens** : 50k (Simple) vs 100k (Complex)
  - **Checkpoint 50k** : OBLIGATOIRE modes Complex (pr√©vention d√©rive)

**Synergie** : Les modes Flash/Mini ont des budgets tokens limit√©s (50k-200k), n√©cessitant d√©l√©gation agressive. Les modes SOTA ont des budgets √©tendus (200k+), permettant grounding exhaustif.

> üí° **Recommandation** : Lire [`llm-modes-mapping.md`](llm-modes-mapping.md) Section 6 pour strat√©gies optimisation par tier.

## üö® Changements Majeurs v2.0.0

Cette version introduit un changement **CRITIQUE** de philosophie :

### ‚ùå Version 1.0.0 (OBSOL√àTE - Risqu√©e)
- Pattern 5 : "Lecture Cibl√©e" encourageait lectures partielles
- √âconomie tokens via fragmentation fichiers
- Anti-pattern : Lecture int√©grale = MAUVAIS
- R√©sultat : Angles morts ‚Üí Bugs subtils ‚Üí Re-travail co√ªteux

### ‚úÖ Version 2.0.0 (ACTUELLE - S√ªre)
- **R√àGLE D'OR** : Lecture compl√®te OBLIGATOIRE avant analyses partielles
- Pattern 6 : "Anti-Angles-Morts" comme principe fondamental
- √âconomie tokens via D√âL√âGATION, pas fragmentation
- Lecture compl√®te = ‚úÖ BON, Lecture fragment√©e = ‚ùå MAUVAIS
- R√©sultat : Contexte complet ‚Üí D√©cisions justes ‚Üí ROI 300%+

**‚ö†Ô∏è IMPORTANT** : Toute r√©f√©rence √† "lecture cibl√©e" ou "lecture partielle" dans v1.0.0 doit √™tre consid√©r√©e comme **OBSOL√àTE et DANGEREUSE**.

---

## üéØ Objectif des Patterns d'√âconomie

L'√©conomie de contexte permet de :
1. **Pr√©server capacit√© cognitive** : Tokens disponibles pour raisonnement
2. **√âviter saturation** : Pr√©venir d√©passement limites contexte
3. **Optimiser performance** : R√©duire latence et co√ªts
4. **Maintenir qualit√©** : Concentration sur t√¢ches essentielles

**Seuils critiques universels** :
- üü¢ 0-30k tokens : Optimal, pleine capacit√©
- üü° 30k-50k tokens : Attention, optimisation recommand√©e
- üü† 50k-100k tokens : Critique, d√©l√©gation obligatoire
- üî¥ >100k tokens : Maximum, orchestration OBLIGATOIRE

---

## üèóÔ∏è Architecture Patterns d'√âconomie

### üéØ Principe Fondamental : Vraie √âconomie = D√©l√©gation

**La vraie √©conomie de tokens ne vient PAS de la lecture partielle (risqu√©e), mais de la D√âL√âGATION (s√ªre).**

- ‚úÖ **D√©l√©gation** : Cr√©er sous-t√¢ches atomiques via `new_task()` ‚Üí Chaque sous-t√¢che lit ses fichiers COMPL√àTEMENT
- ‚ùå **Fragmentation** : Lire fichiers par morceaux ‚Üí Angles morts ‚Üí Re-travail co√ªteux

**Hi√©rarchie d'Utilisation** :
1. **Pattern 1 (D√©l√©gation)** : 80% des cas ‚Üí Solution privil√©gi√©e universelle
2. **Pattern 2 (D√©composition)** : Pr√©paration d√©l√©gation ‚Üí Planification atomique
3. **Pattern 3 (MCP Batch)** : Optimisation op√©rations ‚Üí R√©duction overhead
4. **Pattern 4 (Checkpoints)** : Sauvegarde progression ‚Üí Reprise facilit√©e
5. **Pattern 5 (Lecture Intelligente)** : Lecture compl√®te obligatoire ‚Üí Analyses compl√©mentaires optionnelles
6. **Pattern 6 (Anti-Angles-Morts)** : Principe transversal ‚Üí Application universelle

### Diagramme Hi√©rarchie

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚≠ê PATTERN 1 : D√âL√âGATION INTELLIGENTE (PRIORITAIRE)    ‚îÇ
‚îÇ 80% cas ‚Üí new_task() pour sous-t√¢ches atomiques         ‚îÇ
‚îÇ Vraie √©conomie via parall√©lisation, PAS fragmentation   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PATTERN 2 : D√âCOMPOSITION ATOMIQUE                      ‚îÇ
‚îÇ T√¢ches complexes ‚Üí Sous-t√¢ches minimales               ‚îÇ
‚îÇ Pr√©pare d√©l√©gation efficace                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PATTERN 3 : UTILISATION MCP BATCH                       ‚îÇ
‚îÇ Op√©rations multiples ‚Üí Requ√™tes consolid√©es            ‚îÇ
‚îÇ Optimisation overhead, pas substitut d√©l√©gation        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PATTERN 4 : CHECKPOINTS SYNTH√âTIQUES                    ‚îÇ
‚îÇ Contexte long ‚Üí Synth√®ses r√©guli√®res                   ‚îÇ
‚îÇ Sauvegarde progression, facilite reprise               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PATTERN 5 : LECTURE INTELLIGENTE PROGRESSIVE            ‚îÇ
‚îÇ Lecture compl√®te OBLIGATOIRE ‚Üí Analyses optionnelles   ‚îÇ
‚îÇ Anti-angles-morts : Contexte complet = Non-n√©gociable  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üõ°Ô∏è PATTERN 6 : ANTI-ANGLES-MORTS (TRANSVERSAL)         ‚îÇ
‚îÇ Principe fondamental applicable √† TOUS patterns         ‚îÇ
‚îÇ Contexte complet AVANT toute d√©cision/modification      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üìä R√©partition Usage Recommand√©e

| Pattern | Usage Typique | Cas Application |
|---------|---------------|-----------------|
| 1. D√©l√©gation | **80%** | Toute t√¢che >30k tokens, fichiers multiples, impl√©mentation complexe |
| 2. D√©composition | **70%** | Pr√©paration d√©l√©gation, planning t√¢ches complexes |
| 3. MCP Batch | **40%** | Lectures multiples, √©ditions coordonn√©es, recherches |
| 4. Checkpoints | **30%** | T√¢ches >30k tokens, sessions longues |
| 5. Lecture Intelligente | **100%** | TOUT fichier lu (compl√®tement d'abord) |
| 6. Anti-Angles-Morts | **100%** | TOUTE d√©cision/modification (contexte complet) |

**Note** : Les pourcentages >100% indiquent patterns combinables/transversaux.

---

## 1Ô∏è‚É£ PATTERN : D√©l√©gation Intelligente

### Principe

Le mode **Complex** analyse et con√ßoit, puis **d√©l√®gue l'ex√©cution** au mode **Simple** via `new_task()`.

### √âconomie Typique

**Sans d√©l√©gation** :
```markdown
Mode Complex :
1. Analyse architecture (10k tokens)
2. Lecture 15 fichiers (25k tokens)
3. Conception solution (8k tokens)
4. Impl√©mentation (12k tokens)
5. Tests (10k tokens)
TOTAL : 65k tokens ‚Üí SEUIL CRITIQUE D√âPASS√â
```

**Avec d√©l√©gation** :
```markdown
Mode Complex :
1. Analyse architecture (10k tokens)
2. Conception solution (8k tokens)
3. Cr√©ation 3 sous-t√¢ches (3k tokens)
TOTAL : 21k tokens ‚Üí OPTIMAL ‚úÖ

Mode Simple (3 instances parall√®les) :
- Sous-t√¢che 1 : Impl√©mentation Module A (15k tokens)
- Sous-t√¢che 2 : Impl√©mentation Module B (18k tokens)
- Sous-t√¢che 3 : Tests int√©gration (12k tokens)
TOTAL : 45k tokens r√©partis ‚Üí OPTIMAL ‚úÖ
```

**√âconomie** : 65k ‚Üí 21k + (45k/3) = 36k tokens max dans un contexte unique

### Template D√©l√©gation

```xml
<new_task>
<mode>code-simple</mode>
<message>üéØ **Sous-t√¢che 1.2.1 : Impl√©mentation Module Authentification**

**Contexte Architecture (Synth√©tique)** :
Architecture JWT valid√©e avec endpoints /login, /refresh, /logout.
D√©pendances approuv√©es : bcrypt@5.1.1, jsonwebtoken@9.0.2.

**Sp√©cification Technique D√©taill√©e** :
```typescript
// Interface AuthService attendue
interface AuthService {
  login(email: string, password: string): Promise<{token: string, refresh: string}>;
  refreshToken(token: string): Promise<{newToken: string}>;
  logout(token: string): Promise<{success: boolean}>;
}
```

**Fichiers √† Cr√©er** :
- src/services/AuthService.ts (impl√©mentation compl√®te)
- src/tests/AuthService.test.ts (tests unitaires 100% coverage)

**Crit√®res Validation** :
- ‚úÖ Toutes m√©thodes impl√©ment√©es selon interface
- ‚úÖ Tests unitaires passent (jest)
- ‚úÖ Gestion erreurs compl√®te (try/catch)
- ‚úÖ Documentation JSDoc sur chaque m√©thode

**Ce qui N'EST PAS dans cette sous-t√¢che** :
- Int√©gration API routes (sous-t√¢che 1.2.2)
- Configuration middleware (sous-t√¢che 1.2.3)
- Tests E2E (sous-t√¢che 1.3)

**Estimation** : ~15k tokens, 1h30, complexit√© mod√©r√©e
</message>
</new_task>
```

**Cl√©s du succ√®s** :
- ‚úÖ Contexte synth√©tique (pas de r√©p√©tition analyse compl√®te)
- ‚úÖ Sp√©cification technique pr√©cise (pas d'ambigu√Øt√©)
- ‚úÖ Scope clairement d√©limit√© (pas de d√©rive)
- ‚úÖ Crit√®res validation explicites (pas de va-et-vient)

### Cas d'Usage Typiques

**D√©l√©gation Lecture Batch** :
```xml
<new_task>
<mode>code-simple</mode>
<message>üéØ **Sous-t√¢che 2.1.1 : Extraction Configurations 15 Fichiers**

**Objectif** : Lire et extraire toutes configurations JSON environnements.

**Fichiers** :
- config/env/*.json (10 fichiers)
- config/services/*.json (5 fichiers)

**Format Sortie Attendu** :
```json
{
  "environments": { "dev": {...}, "staging": {...}, "prod": {...} },
  "services": { "api": {...}, "db": {...}, "cache": {...} }
}
```

**Outil** : Utiliser `quickfiles read_multiple_files` pour efficacit√©.

**Estimation** : ~8k tokens, 30 min
</message>
</new_task>
```

**√âconomie** : Mode Complex pr√©serve 8k tokens pour analyse

---

## 2Ô∏è‚É£ PATTERN : D√©composition Atomique

### Principe

D√©composer t√¢ches complexes en **sous-t√¢ches atomiques** (1 responsabilit√©, 1 livrable, 1 validation).

### Granularit√© Optimale

**Trop Grossi√®re** (Anti-pattern) :
```markdown
1.1 Impl√©menter module utilisateurs (50k tokens)
    ‚Üí Trop vaste, contexte satur√©
```

**Trop Fine** (Anti-pattern) :
```markdown
1.1.1 Cr√©er interface User (2k tokens)
1.1.2 Cr√©er classe User (2k tokens)
1.1.3 Ajouter m√©thode getEmail (1k tokens)
1.1.4 Ajouter m√©thode setEmail (1k tokens)
    ‚Üí 4 contextes s√©par√©s, overhead orchestration
```

**Optimale** (Best Practice) :
```markdown
1.1 Impl√©menter entit√© User compl√®te (15k tokens)
    - Interface + Classe + M√©thodes + Tests
    - Scope atomique, livrable testable
    ‚úÖ 1 contexte, efficace
```

### R√®gles de D√©composition

**Crit√®re SMART Atomique** :
- **S**pecific : 1 responsabilit√© claire
- **M**easurable : Crit√®res validation objectifs
- **A**chievable : R√©alisable en <20k tokens
- **R**elevant : Contribue directement objectif parent
- **T**ime-boxed : Estimation dur√©e <2h

**Seuils recommand√©s** :
- Sous-t√¢che Simple : 10-20k tokens, 1-2h
- Sous-t√¢che Mod√©r√©e : 20-35k tokens, 2-4h
- Sous-t√¢che Complexe : 35-50k tokens, 4-6h
- Au-del√† 50k : D√âCOMPOSER ENCORE

### Template D√©composition

**T√¢che Parent (Analyse)** :
```markdown
## T√¢che 1 : Syst√®me Authentification Complet

### Analyse Complexit√©
- 5 composants majeurs identifi√©s
- Estimation totale : ~120k tokens (CRITIQUE)
- D√©composition obligatoire en 5 sous-t√¢ches

### Sous-t√¢ches Atomiques Planifi√©es
1.1 Entit√© User + Repository (20k tokens, 2h)
1.2 Service Authentification JWT (25k tokens, 3h)
1.3 Middleware Validation Token (15k tokens, 1.5h)
1.4 API Routes + Controllers (30k tokens, 3.5h)
1.5 Tests E2E + Documentation (30k tokens, 3h)

TOTAL D√âCOMPOS√â : 120k tokens sur 5 contextes
MAX PAR CONTEXTE : 30k tokens ‚úÖ OPTIMAL
```

**Cr√©ation Sous-t√¢ches** :
```xml
<!-- R√©p√©ter pour chaque sous-t√¢che -->
<new_task>
<mode>code-simple</mode>
<message>üéØ **Sous-t√¢che {{NUMERO}} : {{TITRE}}**

**Contexte H√©rit√© Synth√©tique** :
[D√©cisions architecturales cl√©s, pas d'analyse compl√®te]

**Objectif Atomique** :
[1 responsabilit√©, 1 livrable]

**D√©pendances** :
- Requiert : {{T√ÇCHES_PR√âALABLES}} (doivent √™tre COMPL√âT√âES)
- Produit pour : {{T√ÇCHES_SUIVANTES}}

**Estimation** : {{TOKENS}}k tokens, {{HEURES}}h
</message>
</new_task>
```

---

## 3Ô∏è‚É£ PATTERN : Utilisation MCP Batch

### Principe

Utiliser MCPs pour **op√©rations batch** au lieu d'outils natifs multiples.

### √âconomie Typique

**Sans MCP Batch** (Anti-pattern) :
```xml
<!-- 5 requ√™tes s√©par√©es = 5√ó overhead -->
<read_file><path>module-a.ts</path></read_file>
<read_file><path>module-b.ts</path></read_file>
<read_file><path>module-c.ts</path></read_file>
<read_file><path>module-d.ts</path></read_file>
<read_file><path>module-e.ts</path></read_file>
```

**Avec MCP Batch** (Best Practice) :
```xml
<!-- 1 requ√™te = 1√ó overhead -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>read_multiple_files</tool_name>
<arguments>
{
  "paths": ["module-a.ts", "module-b.ts", "module-c.ts", "module-d.ts", "module-e.ts"],
  "max_lines_per_file": 500
}
</arguments>
</use_mcp_tool>
```

**√âconomie** : 
- Overhead requ√™tes : 5√ó ‚Üí 1√ó (~80% r√©duction)
- Tokens formatting : 5√ó ‚Üí 1√ó (~80% r√©duction)
- Temps r√©ponse : 5√ó latence ‚Üí 1√ó latence

### Patterns MCP Optimaux

#### Pattern A : Lecture Batch Progressive

```markdown
**√âtape 1** : Explorer structure (minimal)
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>list_directory_contents</tool_name>
<arguments>
{
  "paths": ["src/modules"],
  "recursive": false
}
</arguments>
</use_mcp_tool>

**√âtape 2** : Lire fichiers pertinents identifi√©s (batch)
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>read_multiple_files</tool_name>
<arguments>
{
  "paths": ["src/modules/auth.ts", "src/modules/user.ts"],
  "max_lines_per_file": 300
}
</arguments>
</use_mcp_tool>

**√âtape 3** : Lecture approfondie si n√©cessaire (cibl√©e)
<read_file>
<path>src/modules/auth.ts</path>
<line_range>150-250</line_range>
</read_file>
```

**B√©n√©fices** :
- Exploration rapide sans surcharge
- Lecture batch des candidats
- Approfondissement cibl√© minimal

#### Pattern B : √âdition Batch Coordonn√©e

```xml
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>edit_multiple_files</tool_name>
<arguments>
{
  "files": [
    {
      "path": "src/module-a.ts",
      "diffs": [
        {"search": "old import", "replace": "new import", "start_line": 5}
      ]
    },
    {
      "path": "src/module-b.ts",
      "diffs": [
        {"search": "old import", "replace": "new import", "start_line": 8}
      ]
    },
    {
      "path": "src/module-c.ts",
      "diffs": [
        {"search": "old import", "replace": "new import", "start_line": 3}
      ]
    }
  ]
}
</arguments>
</use_mcp_tool>
```

**B√©n√©fices** :
- Atomicit√© (tout ou rien)
- Coh√©rence garantie
- √âconomie 3√ó requ√™tes

#### Pattern C : Recherche Multi-Fichiers

```xml
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["src", "tests"],
  "pattern": "function authenticate",
  "use_regex": true,
  "context_lines": 5,
  "max_results_per_file": 3
}
</arguments>
</use_mcp_tool>
```

**B√©n√©fices** :
- Contexte imm√©diat (lignes encadrantes)
- Statistiques par fichier
- Limites configurables

---

## 4Ô∏è‚É£ PATTERN : Checkpoints Synth√©tiques

### Principe

√Ä intervalles r√©guliers, cr√©er **synth√®ses condens√©es** pour lib√©rer contexte.

### Seuils de Checkpoint

- **30k tokens** : Checkpoint recommand√© (pr√©vention)
- **50k tokens** : Checkpoint OBLIGATOIRE (critique)
- **100k tokens** : Checkpoint + D√âL√âGATION (maximum)

### Format Checkpoint Standard

```markdown
## üîñ Checkpoint 50k Tokens - [Date] [Heure]

### Contexte Actuel
**T√¢che** : 1.2 Impl√©mentation Service Authentification
**Progression** : 60% (~30k tokens consomm√©s)
**Prochaine √©tape** : Tests unitaires

### Actions Accomplies (Synth√®se)
1. ‚úÖ Interface AuthService d√©finie (3 m√©thodes)
2. ‚úÖ Classe AuthService impl√©ment√©e (login, refresh, logout)
3. ‚úÖ Int√©gration bcrypt + jsonwebtoken valid√©e
4. ‚úÖ Gestion erreurs compl√®te (try/catch)

### D√©cisions Techniques Cl√©s
- **JWT Expiration** : 1h access, 7j refresh (valid√© utilisateur)
- **Hash Rounds** : bcrypt rounds=12 (s√©curit√© vs performance)
- **Error Handling** : Exceptions custom AuthError, InvalidTokenError

### √âtat Code Actuel
**Fichiers modifi√©s** :
- `src/services/AuthService.ts` : 150 lignes (complet)
- `src/types/auth.types.ts` : 45 lignes (interfaces)

**R√©sum√© impl√©mentation** :
```typescript
class AuthService {
  async login(email, password) { /* bcrypt verify + JWT sign */ }
  async refreshToken(token) { /* JWT verify + reissue */ }
  async logout(token) { /* Invalidate token */ }
}
```

### Prochaines √âtapes (Priorit√©s)
1. [ ] Cr√©er AuthService.test.ts (tests unitaires complets)
2. [ ] Valider coverage 100% (jest --coverage)
3. [ ] Documentation JSDoc m√©thodes
4. [ ] Int√©gration API routes (sous-t√¢che 1.2.2)

### Recommandations √âconomie Contexte
**Option A** : Continuer dans contexte actuel (30k restants suffisants pour tests)
**Option B** : Cr√©er sous-t√¢che tests si ajout fonctionnalit√©s pr√©vu

**D√©cision** : Option A retenue (tests simples, context OK)
```

**B√©n√©fices** :
- üß† M√©moire externe projet
- üîÑ Reprise facilit√©e apr√®s interruption
- üìä M√©triques progression pr√©cises
- üéØ Focalisation prochaines √©tapes

### Template Checkpoint Automatique

```markdown
## üîñ Checkpoint {{TOKENS}}k - {{TIMESTAMP}}

### Synth√®se 3-Points
1. **Fait** : [R√©sum√© ultra-condens√© actions]
2. **D√©cid√©** : [D√©cisions architecturales cl√©s]
3. **Suivant** : [Prochaines 2-3 √©tapes prioritaires]

### √âtat Fichiers (Delta)
- Modifi√©s : {{LISTE}} (+{{LIGNES_AJOUT√âES}}/-{{LIGNES_SUPPRIM√âES}})
- Cr√©√©s : {{LISTE}}
- Tests : {{STATUT}}

### Recommandation Contexte
{{#if TOKENS > 50}}
üî¥ CRITIQUE : D√©l√©gation recommand√©e
{{else if TOKENS > 30}}
üü° ATTENTION : Checkpoint effectu√©, optimiser suite
{{else}}
üü¢ OPTIMAL : Continue dans contexte actuel
{{/if}}
```

---

## 5Ô∏è‚É£ PATTERN : Lecture Intelligente Progressive

### ‚ö†Ô∏è R√àGLE D'OR ANTI-ANGLES-MORTS

**Toujours lire un fichier EN ENTIER avant toute analyse partielle.**

Cette r√®gle est **NON-N√âGOCIABLE** pour garantir :
- ‚úÖ Compr√©hension contexte global complet
- ‚úÖ Identification de toutes les d√©pendances
- ‚úÖ Pr√©vention des angles morts critiques
- ‚úÖ D√©cisions √©clair√©es bas√©es sur contexte complet

### Principe Fondamental

La lecture compl√®te d'un fichier est **OBLIGATOIRE** comme premi√®re √©tape. Les analyses partielles ou cibl√©es ne sont que des **compl√©ments optionnels** APR√àS avoir obtenu le contexte global.

**‚ùå INTERDIT** : Commencer par des lectures partielles, line_range, ou recherches sans contexte
**‚úÖ OBLIGATOIRE** : Lecture compl√®te d'abord, analyses compl√©mentaires ensuite SI N√âCESSAIRE

### Strat√©gie Obligatoire

**Niveau 1 : Lecture Compl√®te (OBLIGATOIRE - √âtape de Grounding)** :
```xml
<!-- TOUJOURS commencer par ceci -->
<read_file>
<path>src/module.ts</path>
</read_file>
```

**Objectif** :
- Grounding complet sur le fichier
- Compr√©hension architecture et structure
- Identification zones int√©r√™t pour analyses compl√©mentaires
- Pr√©vention angles morts critiques

**R√©sultat** : Base solide pour toute d√©cision ou modification

**Niveau 2 : Analyses Compl√©mentaires (OPTIONNEL - Si N√©cessaire)** :

Apr√®s avoir lu le fichier complet, vous pouvez SI N√âCESSAIRE :

```xml
<!-- Recherche regex cibl√©e pour validation crois√©e -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["src/module.ts"],
  "pattern": "function authenticate",
  "context_lines": 3
}
</arguments>
</use_mcp_tool>

<!-- Re-lecture zone sp√©cifique pour focus -->
<read_file>
<path>src/module.ts</path>
<line_range>150-200</line_range>
</read_file>
```

**Important** : Ces analyses NE REMPLACENT PAS la lecture compl√®te initiale, elles la COMPL√àTENT.

### Vraie √âconomie : D√©l√©gation, Pas Fragmentation

**‚ùå FAUSSE √âCONOMIE (Risqu√©e)** :
```markdown
Lire fichier par fragments pour "√©conomiser tokens"
‚Üí Risque angles morts ‚Üí Re-travail ‚Üí Co√ªt r√©el SUP√âRIEUR
```

**‚úÖ VRAIE √âCONOMIE (S√ªre)** :
```markdown
Lecture compl√®te fichier unique : OK dans contexte actuel
Lecture multiples fichiers : D√âL√âGATION sous-t√¢ches atomiques
‚Üí Chaque sous-t√¢che = 1 fichier lu COMPL√àTEMENT
‚Üí √âconomie via parall√©lisation, pas fragmentation risqu√©e
```

**Exemple D√©l√©gation Optimale** :
```xml
<new_task>
<mode>code-simple</mode>
<message>üéØ **Sous-t√¢che 3.1 : Analyse Module Authentification**

**Objectif** : Lire et analyser COMPL√àTEMENT le module d'authentification.

**Fichier** : src/modules/auth.ts (lecture INT√âGRALE requise)

**Livrable** :
- Synth√®se architecture du module
- Liste d√©pendances identifi√©es
- Points attention pour int√©gration

**Note** : Lecture compl√®te OBLIGATOIRE pour √©viter angles morts.
</message>
</new_task>
```

### Pattern Markdown Structure (Apr√®s Lecture Compl√®te)

Pour documentation volumineuse, la structure peut aider APR√àS lecture compl√®te :

```xml
<!-- √âtape 1 : TOUJOURS lire document complet d'abord -->
<read_file>
<path>docs/architecture.md</path>
</read_file>

<!-- √âtape 2 (optionnel) : Extraire structure pour navigation -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>extract_markdown_structure</tool_name>
<arguments>
{
  "paths": ["docs/architecture.md"],
  "max_depth": 3,
  "include_context": false
}
</arguments>
</use_mcp_tool>
```

**Ordre CRITIQUE** : Lecture compl√®te AVANT structure, jamais l'inverse

---

## üìä Matrice D√©cision √âconomie

### Flowchart Optimisation

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tokens actuels? ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ <30k?  ‚îÇ
    ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îò
      ‚îÇ    ‚îÇ
     OUI  NON
      ‚îÇ    ‚îÇ
      ‚Üì    ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ OK  ‚îÇ ‚îÇ 30-50k?  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îò
             ‚îÇ      ‚îÇ
            OUI    NON
             ‚îÇ      ‚îÇ
             ‚Üì      ‚Üì
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇCHECKPOINT‚îÇ ‚îÇ >50k?    ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îò
                     ‚îÇ      ‚îÇ
                    OUI    NON
                     ‚îÇ      ‚îÇ
                     ‚Üì      ‚Üì
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ D√âL√âGUER  ‚îÇ ‚îÇ >100k?   ‚îÇ
              ‚îÇ new_task()‚îÇ ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îò
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ      ‚îÇ
                             OUI    NON
                              ‚îÇ      ‚îÇ
                              ‚Üì      ‚Üì
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇORCHESTR. ‚îÇ ‚îÇCHECKPOINT‚îÇ
                       ‚îÇOBLIGAT.  ‚îÇ ‚îÇ+ D√âL√âGAT.‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 6Ô∏è‚É£ PATTERN : Anti-Angles-Morts (Pr√©vention Blind Spots)

### üéØ Principe Fondamental

**Les angles morts sont l'ennemi silencieux de la qualit√© logicielle.**

Un "angle mort" survient lorsqu'une d√©cision ou modification est prise sur la base d'un contexte **incomplet** ou **fragment√©**, conduisant √† :
- üêõ Bugs subtils non d√©tect√©s
- üîÑ Re-travail co√ªteux
- üí• R√©gression fonctionnelle
- üß© Incoh√©rences architecturales
- ‚è±Ô∏è Co√ªt total SUP√âRIEUR aux tokens "√©conomis√©s"

### ‚ö†Ô∏è R√®gle Anti-Angles-Morts

```
AVANT toute d√©cision ou modification :
Contexte COMPLET = OBLIGATOIRE

Fichier = Unit√© atomique d'information
‚Üí TOUJOURS lire EN ENTIER
```

### Workflow Obligatoire Anti-Angles-Morts

**√âtape 1 : Identification Fichiers Pertinents** :
```xml
<!-- Recherche s√©mantique pour d√©couverte -->
<codebase_search>
<query>authentication jwt token validation</query>
<path>src</path>
</codebase_search>

<!-- Ou exploration structure -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>list_directory_contents</tool_name>
<arguments>{"paths": ["src/auth"], "recursive": true}</arguments>
</use_mcp_tool>
```

**√âtape 2 : Lecture Compl√®te CHAQUE Fichier Identifi√©** :
```xml
<!-- ‚úÖ CORRECT : Lecture int√©grale -->
<read_file>
<path>src/auth/AuthService.ts</path>
</read_file>

<read_file>
<path>src/auth/TokenValidator.ts</path>
</read_file>
```

**√âtape 3 : Synth√®se Mentale Contexte Global** :
```markdown
Apr√®s lectures compl√®tes, mental model :
- Architecture AuthService utilise TokenValidator
- D√©pendance jwt library version 9.0.2
- Validation tokens inclut refresh + access
- Error handling via custom AuthError
‚Üí Base solide pour modifications
```

**√âtape 4 : Analyses Compl√©mentaires (Si N√©cessaire)** :
```xml
<!-- Validation crois√©e APR√àS contexte complet -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["src"],
  "pattern": "AuthError",
  "context_lines": 5
}
</arguments>
</use_mcp_tool>
```

**√âtape 5 : D√©cisions √âclair√©es** :
```markdown
D√©cision bas√©e sur :
‚úÖ Architecture compl√®te comprise
‚úÖ D√©pendances identifi√©es
‚úÖ Patterns existants respect√©s
‚úÖ Impacts anticip√©s

‚Üí Modifications coh√©rentes et s√ªres
```

### üö´ Anti-Patterns Flagrants

#### ‚ùå Anti-Pattern 1 : Search-First (Recherche Sans Contexte)

**MAUVAIS** :
```xml
<!-- Chercher fonction sans lire fichier complet -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["src/auth/AuthService.ts"],
  "pattern": "function login"
}
</arguments>
</use_mcp_tool>

<!-- Modifier bas√© sur fragment trouv√© -->
<apply_diff>
<path>src/auth/AuthService.ts</path>
<diff>...</diff>
</apply_diff>
```

**Probl√®mes** :
- üî¥ Architecture globale ignor√©e
- üî¥ D√©pendances manqu√©es
- üî¥ Side-effects non anticip√©s
- üî¥ Risque breaking changes

**CORRECT** :
```xml
<!-- 1. Lire fichier COMPLET d'abord -->
<read_file>
<path>src/auth/AuthService.ts</path>
</read_file>

<!-- 2. Comprendre architecture globale -->
<!-- 3. PUIS modifier en connaissance de cause -->
<apply_diff>
<path>src/auth/AuthService.ts</path>
<diff>...</diff>
</apply_diff>
```

#### ‚ùå Anti-Pattern 2 : Line-Range-First (Lecture Partielle)

**MAUVAIS** :
```xml
<!-- Lire uniquement lignes "int√©ressantes" -->
<read_file>
<path>src/config/database.ts</path>
<line_range>50-100</line_range>
</read_file>

<!-- Manque : imports, constantes globales, exports -->
```

**Probl√®mes** :
- üî¥ Imports manqu√©s (d√©pendances inconnues)
- üî¥ Constantes globales ignor√©es
- üî¥ Exports incompris
- üî¥ D√©cision bas√©e sur vision tronqu√©e

**CORRECT** :
```xml
<!-- Lire fichier COMPLET -->
<read_file>
<path>src/config/database.ts</path>
</read_file>

<!-- Optionnel : Re-focus zone si besoin APR√àS -->
<read_file>
<path>src/config/database.ts</path>
<line_range>50-100</line_range>
</read_file>
```

#### ‚ùå Anti-Pattern 3 : Assumption-Driven (Hypoth√®ses Non V√©rifi√©es)

**MAUVAIS** :
```markdown
"Je suppose que cette fonction fait X"
‚Üí Modification bas√©e sur supposition
‚Üí Sans v√©rifier contexte complet
```

**Probl√®mes** :
- üî¥ Hypoth√®ses souvent fausses
- üî¥ Comportements surprenants ignor√©s
- üî¥ Edge cases manqu√©s
- üî¥ Tests cass√©s

**CORRECT** :
```markdown
1. Lire fichier complet
2. Comprendre R√âELLEMENT ce que fait fonction
3. V√©rifier edge cases et error handling
4. PUIS modifier en connaissance compl√®te
```

#### ‚ùå Anti-Pattern 4 : Quick-Fix-Syndrome (Modification Rapide)

**MAUVAIS** :
```markdown
"Je vais juste modifier cette ligne rapidement"
‚Üí Sans lire reste du fichier
‚Üí Sans comprendre contexte global
```

**Probl√®mes** :
- üî¥ R√©gression introduite
- üî¥ Incoh√©rences avec reste code
- üî¥ Breaking changes non d√©tect√©s
- üî¥ Co√ªt re-travail > temps "gagn√©"

**CORRECT** :
```markdown
1. M√™me pour "petite" modification : Lire fichier complet
2. Comprendre impact potentiel
3. V√©rifier coh√©rence avec architecture
4. Modifier en s√©curit√©
```

### ‚úÖ Exceptions Rares (Usage Avanc√©)

**Exception 1 : Fichiers Volumineux >10k Lignes** :
```xml
<!-- OK : Lecture compl√®te + focus zones -->
<read_file>
<path>src/generated/api-client.ts</path>
</read_file>

<!-- 15000 lignes lues, focus sur zone modifiable -->
<read_file>
<path>src/generated/api-client.ts</path>
<line_range>8000-8500</line_range>
</read_file>
```

**Justification** : Contexte global obtenu, re-lecture zone pour clart√©

**Exception 2 : Logs Volumineux** :
```xml
<!-- OK : Strat√©gie logs d√©finie AVANT recherche -->
<read_file>
<path>logs/app.log</path>
<line_range>-1000</line_range> <!-- 1000 derni√®res lignes -->
</read_file>

<!-- Recherche patterns erreurs -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["logs/app.log"],
  "pattern": "ERROR|FATAL",
  "context_lines": 10
}
</arguments>
</use_mcp_tool>
```

**Justification** : Logs = nature diff√©rente (s√©quentiel, non-architectural)

**Exception 3 : Data Files (CSV, JSON volumineux)** :
```xml
<!-- OK : Headers + √©chantillon PUIS recherche -->
<read_file>
<path>data/users.csv</path>
<line_range>1-100</line_range> <!-- Headers + premiers records -->
</read_file>

<!-- Recherche record sp√©cifique si besoin -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["data/users.csv"],
  "pattern": "user-12345"
}
</arguments>
</use_mcp_tool>
```

**Justification** : Data tabulaire = structure r√©p√©titive, √©chantillon suffit

### üìö Cas d'√âcole : Incident README.md

**Contexte R√©el** :
```markdown
Utilisateur demande : "Analyse le README.md du projet"

‚ùå MA PREMI√àRE TENTATIVE (MAUVAISE) :
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["README.md"],
  "pattern": "installation|configuration|usage"
}
</arguments>
</use_mcp_tool>

üî¥ FEEDBACK UTILISATEUR :
"Lis le fichier EN ENTIER pour ne pas avoir d'angles morts"
```

**Probl√®me Identifi√©** :
- Recherche fragment√©e a rat√© sections importantes
- Vision tronqu√©e du projet
- Recommandations bas√©es sur contexte incomplet
- Angles morts sur architecture et d√©pendances

**Correction** :
```xml
<read_file>
<path>README.md</path>
</read_file>
```

**R√©sultat** :
- ‚úÖ Contexte complet obtenu
- ‚úÖ Toutes sections comprises (m√™me non search√©es)
- ‚úÖ Recommandations pertinentes et coh√©rentes
- ‚úÖ Z√©ro angle mort

**Le√ßon** : **1 fichier = 1 lecture compl√®te, TOUJOURS**

### üí∞ Co√ªt R√©el : Angles Morts vs Tokens "√âconomis√©s"

**Sc√©nario Typique** :

```markdown
### Sans Anti-Angles-Morts (Lecture Partielle)
1. Lecture partielle fichier (line_range) : 2k tokens √©conomis√©s
2. Modification bas√©e sur contexte incomplet
3. Tests √©chouent ‚Üí Re-lecture compl√®te : 5k tokens
4. D√©couverte d√©pendance manqu√©e ‚Üí Re-lecture autre fichier : 3k tokens
5. Correction modification : 2k tokens
6. Re-tests : 1k tokens

TOTAL R√âEL : 13k tokens + 3h de re-travail
√âCONOMIE INITIALE : 2k tokens
CO√õT NET : +11k tokens + 3h perdues ‚ùå

### Avec Anti-Angles-Morts (Lecture Compl√®te)
1. Lecture compl√®te fichier d'embl√©e : 5k tokens
2. Contexte complet = d√©cision √©clair√©e
3. Modification correcte premier coup : 2k tokens
4. Tests passent : 1k tokens

TOTAL : 8k tokens + 1h travail efficace
√âCONOMIE NETTE : 5k tokens + 2h gagn√©es ‚úÖ
```

**Conclusion Math√©matique** :
```
Lecture compl√®te d'embl√©e = MOINS co√ªteuse que lecture partielle + re-travail

Angles morts = Fausse √©conomie
Contexte complet = Vraie √©conomie
```

### üéØ Impact sur Qualit√© D√©cisions

**D√©cisions avec Angles Morts** :
- üî¥ Bas√©es sur hypoth√®ses
- üî¥ Ignorent cas edge
- üî¥ Ratent d√©pendances critiques
- üî¥ Incoh√©rentes avec architecture existante
- üî¥ N√©cessitent corrections multiples

**D√©cisions avec Contexte Complet** :
- ‚úÖ Bas√©es sur faits v√©rifi√©s
- ‚úÖ Anticipent cas edge
- ‚úÖ Respectent d√©pendances
- ‚úÖ Coh√©rentes avec architecture
- ‚úÖ Correctes premier coup

**M√©triques Qualit√©** :
```
Taux succ√®s premier coup :
- Avec angles morts : ~40%
- Avec contexte complet : ~85%

Temps moyen r√©solution :
- Avec angles morts : 3-5h (it√©rations multiples)
- Avec contexte complet : 1-2h (direct)
```

### üõ°Ô∏è R√©sum√© Ex√©cutif

**3 R√®gles d'Or** :
1. **Fichier = Unit√© Atomique** : Toujours lire en entier
2. **Contexte Avant Action** : Comprendre avant modifier
3. **D√©l√©gation Si Multiple** : N fichiers = N sous-t√¢ches (chacune lit 1 fichier complet)

**Formule Succ√®s** :
```
Qualit√© Code = f(Contexte Complet)
Contexte Complet ‚áí Lecture Int√©grale Fichiers
‚à¥ Lecture Int√©grale = Non-n√©gociable
```

**Rappel** : Les tokens "√©conomis√©s" par lectures partielles sont une **illusion** si le co√ªt du re-travail est pris en compte. La vraie √©conomie passe par la **d√©l√©gation**, pas la fragmentation.


### Tableau Recommandations

| Tokens | √âtat | Action Imm√©diate | Pattern Recommand√© |
|--------|------|------------------|-------------------|
| 0-10k | üü¢ D√©marrage | Aucune | Exploration normale |
| 10-30k | üü¢ Optimal | Aucune | Continue, MCP batch si opportun |
| 30-40k | üü° Attention | Checkpoint pr√©ventif | Patterns 3-5 (Batch, Lecture cibl√©e) |
| 40-50k | üü° Vigilance | Checkpoint + Analyse | Patterns 1-2 (D√©l√©gation pr√©par√©e) |
| 50-70k | üü† Critique | D√©l√©gation OBLIGATOIRE | Pattern 1-2 (new_task atomiques) |
| 70-100k | üü† Maximum | D√©l√©gation + Checkpoint | Pattern 2 + 4 (D√©composition + Synth√®se) |
| >100k | üî¥ D√©passement | Orchestration IMM√âDIATE | Escalade orchestrator-complex |

---

## üé® Templates Instructions Modes

### Template Mode Simple

```markdown
## √âCONOMIE CONTEXTE

### Monitoring Tokens
√Ä chaque outil use, √©valuer :
- Tokens consomm√©s approximatifs
- Total cumul√© actuel
- Distance seuil suivant (30k, 50k)

### Actions Pr√©ventives
**Si approche 30k** :
- Checkpoint synth√©tique
- Prioriser t√¢ches restantes
- √âvaluer possibilit√© finalisation

**Si approche 50k** :
- Checkpoint OBLIGATOIRE
- Escalade vers mode complexe si t√¢che incompl√®te
- Ou finalisation rapide si quasi-termin√©

### Utilisation MCP Efficace
- Privil√©gier `quickfiles` pour op√©rations batch
- Lecture cibl√©e (line_range) au lieu de compl√®te
- √âviter r√©p√©titions requ√™tes similaires
```

### Template Mode Complex

```markdown
## √âCONOMIE CONTEXTE AVANC√âE

### Patterns D√©l√©gation
**D√®s 30k tokens** : √âvaluer d√©composition
**D√®s 50k tokens** : D√©l√©gation OBLIGATOIRE

### Cr√©ation Sous-t√¢ches Atomiques
```xml
<new_task>
<mode>code-simple</mode>
<message>üéØ **Sous-t√¢che {{NUMERO}} : {{TITRE}}**

**Contexte Synth√©tique** : [Essentiel seulement]
**Objectif Atomique** : [1 responsabilit√©]
**Estimation** : {{TOKENS}}k tokens, {{HEURES}}h
</message>
</new_task>
```

### Checkpoints R√©guliers
- 30k : Checkpoint pr√©ventif
- 50k : Checkpoint + D√©l√©gation √©valu√©e
- 70k : Checkpoint + D√©l√©gation OBLIGATOIRE

### Strat√©gie Analyse puis D√©l√©gation
1. Analyser et concevoir (garder contexte riche)
2. Cr√©er sous-t√¢ches impl√©mentation (contexte synth√©tique)
3. Modes simples ex√©cutent (contextes l√©gers)
4. Valider et int√©grer (contexte pr√©serv√©)
```

---

## ‚ö†Ô∏è Anti-Patterns √† √âviter

### ‚úÖ Lecture Compl√®te d'Abord (Grounding Obligatoire)
```xml
<!-- BON : Lecture int√©grale pour contexte complet -->
<read_file>
<path>module.ts</path>
</read_file>

<!-- Puis analyses compl√©mentaires SI N√âCESSAIRE -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["module.ts"],
  "pattern": "function authenticate",
  "context_lines": 3
}
</arguments>
</use_mcp_tool>
```

**Justification** :
- ‚úÖ Contexte global complet obtenu
- ‚úÖ Pr√©vention angles morts
- ‚úÖ D√©cisions √©clair√©es
- ‚úÖ Z√©ro risque information manqu√©e

### ‚ùå Lecture Fragment√©e Sans Contexte Global
```xml
<!-- MAUVAIS : Recherche ou line_range SANS lecture compl√®te pr√©alable -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["module.ts"],
  "pattern": "function authenticate"
}
</arguments>
</use_mcp_tool>

<!-- Ou pire : Lecture partielle directe -->
<read_file>
<path>module.ts</path>
<line_range>200-250</line_range>
</read_file>

<!-- Sans JAMAIS avoir lu le fichier complet -->
```

**Probl√®mes** :
- üî¥ Angles morts critiques (informations manqu√©es)
- üî¥ D√©cisions bas√©es sur contexte incomplet
- üî¥ Risque bugs subtils et r√©gressions
- üî¥ Co√ªt re-travail > tokens "√©conomis√©s"

**Note Importante** : `list_code_definition_names` est √©galement un anti-pattern si utilis√© SANS lecture compl√®te. Il ne remplace PAS la lecture int√©grale, il peut seulement la COMPL√âTER.

### ‚ùå Pas de Checkpoint Avant Saturation
```markdown
Mode continue jusqu'√† 95k tokens sans checkpoint
‚Üí Pas de point reprise, contexte perdu si erreur
```

### ‚úÖ Checkpoints R√©guliers Pr√©ventifs
```markdown
- 30k : Checkpoint 1 (pr√©vention)
- 50k : Checkpoint 2 (critique)
- 70k : Checkpoint 3 + d√©l√©gation
‚Üí Points reprise multiples, tra√ßabilit√© compl√®te
```

### ‚ùå D√©composition Excessive
```markdown
D√©composition en 50 micro-t√¢ches de 2k tokens chacune
‚Üí Overhead orchestration √©norme, perte vue ensemble
```

### ‚úÖ D√©composition √âquilibr√©e
```markdown
D√©composition en 5-8 sous-t√¢ches atomiques 15-30k chacune
‚Üí Granularit√© optimale, orchestration efficace
```

---

## üìä M√©triques √âconomie Contexte

### ‚ö†Ô∏è Principe M√©trique : Co√ªt Total R√©el

**Les m√©triques doivent inclure le CO√õT COMPLET**, pas seulement tokens consomm√©s :

```
Co√ªt Total R√©el = Tokens Lecture + Tokens Re-travail + Co√ªt Bugs/R√©gressions

Lecture Partielle : Tokens initiaux faibles MAIS re-travail √©lev√©
Lecture Compl√®te : Tokens initiaux moyens MAIS re-travail quasi-nul

‚à¥ Lecture Compl√®te = Co√ªt Total INF√âRIEUR
```

### Par T√¢che (M√©triques Enrichies)

```markdown
## Rapport √âconomie Contexte - T√¢che {{ID}}

### Utilisation Tokens
- Tokens consomm√©s : {{CONSUMED}}k
- Tokens √©conomis√©s (d√©l√©gation) : {{SAVED_DELEGATION}}k
- Tokens √©conomis√©s (MCP batch) : {{SAVED_BATCH}}k
- **Co√ªt angles morts √©vit√©s** : {{SAVED_BLIND_SPOTS}}k
- Efficacit√© nette : {{NET_EFFICIENCY}}%

### Patterns Appliqu√©s (S√ªrs)
- ‚úÖ D√©l√©gation intelligente : {{COUNT_DELEGATION}} sous-t√¢ches
- ‚úÖ MCP Batch : {{COUNT_BATCH}} vs {{COUNT_NATIVE}} natifs
- ‚úÖ Lectures compl√®tes : {{COUNT_FULL_READ}} fichiers

### Patterns Risqu√©s √âvit√©s
- ‚ùå Lectures partielles √©vit√©es : {{COUNT_AVOIDED_PARTIAL}}
- ‚ùå Recherches sans contexte √©vit√©es : {{COUNT_AVOIDED_SEARCH_FIRST}}
- üõ°Ô∏è Angles morts pr√©venus : {{COUNT_BLIND_SPOTS_PREVENTED}}

### Impact Qualit√©
- Modifications correctes 1er coup : {{FIRST_TIME_RIGHT}}%
- Re-travail n√©cessaire : {{REWORK_COUNT}} it√©rations
- Bugs d√©tect√©s post-livraison : {{BUGS_POST_DELIVERY}}
- Temps total (incluant corrections) : {{TOTAL_TIME}}h

### Seuils Franchis
- 30k : {{TIMESTAMP_30K}} (checkpoint pr√©ventif)
- 50k : {{TIMESTAMP_50K}} (+ checkpoint + √©valuation d√©l√©gation)
- 70k : {{TIMESTAMP_70K}} (+ d√©l√©gation OBLIGATOIRE)

### Comparaison Sc√©narios

| Sc√©nario | Tokens Initiaux | Re-travail | Total | Qualit√© |
|----------|-----------------|------------|-------|---------|
| **Lecture Partielle** | {{PARTIAL_INITIAL}}k | {{PARTIAL_REWORK}}k | {{PARTIAL_TOTAL}}k | {{PARTIAL_QUALITY}}% |
| **Lecture Compl√®te** | {{FULL_INITIAL}}k | {{FULL_REWORK}}k | {{FULL_TOTAL}}k | {{FULL_QUALITY}}% |
| **√âconomie R√©elle** | +{{DIFF_INITIAL}}k | -{{DIFF_REWORK}}k | **-{{DIFF_TOTAL}}k** ‚úÖ | +{{DIFF_QUALITY}}% |

### Recommandations Futures
- Patterns s√ªrs qui ont bien fonctionn√© : [D√©l√©gation, Lecture compl√®te, MCP batch]
- Patterns risqu√©s √©vit√©s avec succ√®s : [Lecture partielle, Search-first]
- Am√©lioration continue : [Points sp√©cifiques identifi√©s]
```

### Par Mode (Agr√©g√©)

```markdown
## Rapport Mode {{MODE_SLUG}} - P√©riode {{PERIOD}}

### Statistiques Tokens
- Moyenne par t√¢che : {{AVG_TOKENS}}k
- Maximum atteint : {{MAX_TOKENS}}k
- T√¢ches >50k : {{COUNT_CRITICAL}} ({{PERCENT}}%)

### Efficacit√© Patterns S√ªrs
- D√©l√©gations effectu√©es : {{COUNT_DELEGATIONS}}
- √âconomie moyenne d√©l√©gation : {{AVG_SAVED_DELEGATION}}k tokens/d√©l√©gation
- Taux MCP batch : {{MCP_RATE}}%
- Taux lecture compl√®te : {{FULL_READ_RATE}}% (objectif: 100%)

### Pr√©vention Angles Morts
- Lectures compl√®tes vs partielles : {{FULL_VS_PARTIAL_RATIO}}
- Angles morts √©vit√©s : {{BLIND_SPOTS_AVOIDED}}
- √âconomie re-travail : {{REWORK_SAVED}}k tokens
- Co√ªt bugs pr√©venus : {{BUGS_PREVENTED_COST}}k tokens √©quivalent

### Qualit√© & Fiabilit√©
- T√¢ches compl√©t√©es sans saturation : {{SUCCESS_RATE}}%
- **T√¢ches correctes 1er coup** : {{FIRST_TIME_RIGHT_RATE}}%
- Re-travail moyen : {{AVG_REWORK}} it√©rations/t√¢che
- Escalades forc√©es (>100k) : {{ESCALATIONS}}
- Bugs post-livraison : {{POST_DELIVERY_BUGS}}

### ROI Patterns

| Pattern | Co√ªt Impl√©mentation | √âconomie G√©n√©r√©e | ROI | Risque |
|---------|---------------------|------------------|-----|--------|
| D√©l√©gation | Faible (planning) | **Tr√®s √©lev√©e** (parall√©lisation) | **500%** | ‚úÖ Nul |
| Lecture Compl√®te | Moyen (tokens) | **√âlev√©e** (√©vite re-travail) | **300%** | ‚úÖ Nul |
| MCP Batch | Faible (setup) | Moyenne (overhead) | 200% | ‚úÖ Nul |
| Lecture Partielle | Faible (tokens) | **N√©gative** (re-travail) | **-150%** | üî¥ √âlev√© |

**Conclusion** : Patterns "√©conomes en tokens" peuvent avoir ROI **n√©gatif** si re-travail pris en compte.
```

---

## üöÄ B√©n√©fices √âconomie Contexte (R√©vis√©s)

### B√©n√©fices Quantifiables

1. **Performance Tokens** : -40% tokens moyens par t√¢che (via d√©l√©gation)
2. **Qualit√© Code** : +85% modifications correctes 1er coup (via lecture compl√®te)
3. **Pr√©vention Angles Morts** : -90% bugs subtils (via contexte complet)
4. **R√©duction Re-travail** : -75% it√©rations corrections (via d√©cisions √©clair√©es)
5. **Co√ªts API** : -35% co√ªts globaux (d√©l√©gation + pr√©vention re-travail)
6. **Fiabilit√©** : +80% t√¢ches finalis√©es premier essai
7. **Maintenabilit√©** : Checkpoints facilitent reprises (+60% vitesse reprise)

### Comparaison Approches

**Approche "Fausse √âconomie" (Lecture Partielle)** :
```
Tokens lecture : 2k (√©conomie apparente)
Tokens re-travail : 11k (angles morts)
Temps perdu : 3h
Bugs introduits : 2-3
TOTAL : 13k tokens + 3h + bugs ‚ùå
```

**Approche "Vraie √âconomie" (Lecture Compl√®te + D√©l√©gation)** :
```
Tokens lecture compl√®te : 5k
Tokens re-travail : 1k (d√©cisions justes)
Temps √©conomis√© : 2h
Bugs introduits : 0
TOTAL : 6k tokens + gain 2h + qualit√© ‚úÖ
```

**Gain Net** : 7k tokens + 5h + √©limination bugs = **ROI 300%+**

### Impact Long Terme

- **Confiance Utilisateur** : D√©cisions bas√©es contexte complet ‚Üí Recommandations fiables
- **V√©locit√© Projet** : Moins corrections ‚Üí Progression lin√©aire stable
- **Dette Technique** : Pr√©vention angles morts ‚Üí Code maintenable long terme
- **Apprentissage** : Patterns s√ªrs r√©utilisables ‚Üí Am√©lioration continue

---

## üìö Ressources Compl√©mentaires

- [`escalade-mechanisms-revised.md`](escalade-mechanisms-revised.md) : D√©l√©gation et d√©sescalade √©conomique
- [`hierarchie-numerotee-subtasks.md`](hierarchie-numerotee-subtasks.md) : D√©composition atomique optimale
- [`sddd-protocol-4-niveaux.md`](sddd-protocol-4-niveaux.md) : Grounding initial complet obligatoire
- [`mcp-integrations-priority.md`](mcp-integrations-priority.md) : Utilisation MCP batch efficace
- [`factorisation-commons.md`](factorisation-commons.md) : R√©duction instructions communes

---

## üîó Coh√©rence Architecturale

### Alignement avec SDDD Protocol (4 Niveaux)

Le protocole SDDD impose un **Grounding Initial** obligatoire au d√©but de chaque t√¢che. Cette r√©vision du document [`context-economy-patterns.md`](context-economy-patterns.md) renforce cette exigence :

- **SDDD Niveau 1 (Grounding Initial)** ‚ÜîÔ∏è **Pattern 6 (Anti-Angles-Morts)** : Contexte complet AVANT action
- **SDDD Niveau 2 (Checkpoint Milieu)** ‚ÜîÔ∏è **Pattern 4 (Checkpoints)** : Synth√®ses r√©guli√®res
- **SDDD Niveau 3 (Validation Finale)** ‚ÜîÔ∏è **M√©triques Qualit√©** : V√©rification correctness 1er coup

**Coh√©rence Valid√©e** : ‚úÖ Le grounding complet (lecture int√©grale fichiers) est maintenant **NON-N√âGOCIABLE** dans les deux documents.

### Alignement avec Hi√©rarchie Sous-T√¢ches

Le document [`hierarchie-numerotee-subtasks.md`](hierarchie-numerotee-subtasks.md) d√©finit `new_task()` comme m√©canisme universel de d√©l√©gation. Cette r√©vision positionne la **D√©l√©gation comme Pattern #1 prioritaire (80% des cas)** :

- **D√©composition Atomique** ‚ÜîÔ∏è **Pattern 2** : Sous-t√¢ches 15-30k tokens
- **new_task() Universel** ‚ÜîÔ∏è **Pattern 1 D√©l√©gation** : M√©canisme privil√©gi√©
- **Scope D√©limit√©** ‚ÜîÔ∏è **Lecture Compl√®te** : Chaque sous-t√¢che lit ses fichiers EN ENTIER

**Coh√©rence Valid√©e** : ‚úÖ La d√©l√©gation via `new_task()` est le pattern #1, chaque sous-t√¢che applique principe anti-angles-morts.

### Alignement avec Escalade Mechanisms

Le document [`escalade-mechanisms-revised.md`](escalade-mechanisms-revised.md) clarifie la distinction entre d√©l√©gation (pr√©f√©r√©e) et escalade (dernier recours). Cette r√©vision renforce cette distinction :

- **D√©l√©gation Pr√©ventive** ‚ÜîÔ∏è **Pattern 1** : √âvite saturation contexte
- **Escalade Forc√©e** ‚ÜîÔ∏è **Seuil >100k** : Dernier recours seulement
- **D√©composition vs Escalade** ‚ÜîÔ∏è **Patterns 1-2 vs Switch Mode** : D√©l√©gation privil√©gi√©e

**Coh√©rence Valid√©e** : ‚úÖ D√©l√©gation = pattern prioritaire, escalade = exception rare.

### Principe Transversal Unifi√©

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          R√àGLE D'OR TRANSVERSALE (Tous Documents)          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  Contexte Complet AVANT D√©cision/Action                    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  - SDDD Protocol : Grounding initial obligatoire           ‚îÇ
‚îÇ  - Context Economy : Lecture int√©grale fichiers            ‚îÇ
‚îÇ  - Hi√©rarchie Subtasks : Scope complet par sous-t√¢che      ‚îÇ
‚îÇ  - Escalade Mechanisms : Contexte avant d√©l√©gation         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚ö†Ô∏è Angles Morts = Ennemi Commun Pr√©venu par Tous Patterns ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìù Note Finale R√©vis√©e

**Version 2.0.0 - R√©vision Majeure FB-04**

L'√©conomie de contexte n'est **pas une contrainte** mais une **opportunit√© d'optimisation s√ªre**. Cette r√©vision majeure clarifie un principe fondamental :

### üéØ Message Cl√©

**La VRAIE √©conomie de tokens = D√âL√âGATION, pas lecture partielle risqu√©e.**

### ‚ö†Ô∏è Changements Critiques

1. **Pattern 5 R√©vis√©** : Lecture compl√®te OBLIGATOIRE avant analyses partielles
2. **Pattern 6 Ajout√©** : Anti-Angles-Morts comme principe fondamental
3. **Hi√©rarchie Invers√©e** : D√©l√©gation = Pattern #1 prioritaire (80% cas)
4. **Anti-Patterns Invers√©s** : Lecture compl√®te = ‚úÖ BON, Lecture partielle = ‚ùå MAUVAIS
5. **M√©triques Enrichies** : Co√ªt angles morts inclus dans calculs ROI

### üõ°Ô∏è R√®gle d'Or Universelle

```
AVANT toute d√©cision ou modification :
‚Üí Contexte COMPLET = OBLIGATOIRE
‚Üí Fichier = Unit√© atomique (lire EN ENTIER)
‚Üí Angles morts = Ennemi silencieux √† pr√©venir
```

### ‚úÖ R√©sultat Attendu

- **Qualit√©** : +85% modifications correctes 1er coup
- **Fiabilit√©** : -90% bugs subtils par pr√©vention angles morts
- **ROI** : 300%+ via r√©duction re-travail
- **Confiance** : D√©cisions √©clair√©es sur contexte complet

Les patterns pr√©sent√©s permettent de maintenir **qualit√© ET performance** tout en respectant les limites des mod√®les de langage, en privil√©giant approches **s√ªres** (d√©l√©gation, lecture compl√®te) sur approches **risqu√©es** (lecture partielle, recherche sans contexte).