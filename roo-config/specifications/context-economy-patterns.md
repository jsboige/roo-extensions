# ğŸ’° Patterns d'Ã‰conomie Contexte - Optimisation Tokens

**Version :** 2.0.0 âš ï¸ **RÃ‰VISION MAJEURE FB-04**
**Date :** 2 Octobre 2025
**Architecture :** 2-Niveaux (Simple/Complex)
**Objectif :** Maximiser efficacitÃ© utilisation tokens ET qualitÃ© code
**RÃ©vision :** Principe Anti-Angles-Morts + DÃ©lÃ©gation Prioritaire

### ğŸ”— Lien avec Mapping LLMs

Les patterns d'Ã©conomie contexte sont **adaptÃ©s par tier LLM** :

- **[`llm-modes-mapping.md`](llm-modes-mapping.md)** : DÃ©finit l'optimisation budget par tier
  - **Modes Simples (Flash/Mini)** : Grounding minimal, dÃ©lÃ©gation prioritaire
  - **Modes Complex (SOTA)** : Grounding complet 4-niveaux SDDD
  - **Seuils tokens** : 50k (Simple) vs 100k (Complex)
  - **Checkpoint 50k** : OBLIGATOIRE modes Complex (prÃ©vention dÃ©rive)

**Synergie** : Les modes Flash/Mini ont des budgets tokens limitÃ©s (50k-200k), nÃ©cessitant dÃ©lÃ©gation agressive. Les modes SOTA ont des budgets Ã©tendus (200k+), permettant grounding exhaustif.

> ğŸ’¡ **Recommandation** : Lire [`llm-modes-mapping.md`](llm-modes-mapping.md) Section 6 pour stratÃ©gies optimisation par tier.

## ğŸš¨ Changements Majeurs v2.0.0

Cette version introduit un changement **CRITIQUE** de philosophie :

### âŒ Version 1.0.0 (OBSOLÃˆTE - RisquÃ©e)
- Pattern 5 : "Lecture CiblÃ©e" encourageait lectures partielles
- Ã‰conomie tokens via fragmentation fichiers
- Anti-pattern : Lecture intÃ©grale = MAUVAIS
- RÃ©sultat : Angles morts â†’ Bugs subtils â†’ Re-travail coÃ»teux

### âœ… Version 2.0.0 (ACTUELLE - SÃ»re)
- **RÃˆGLE D'OR** : Lecture complÃ¨te OBLIGATOIRE avant analyses partielles
- Pattern 6 : "Anti-Angles-Morts" comme principe fondamental
- Ã‰conomie tokens via DÃ‰LÃ‰GATION, pas fragmentation
- Lecture complÃ¨te = âœ… BON, Lecture fragmentÃ©e = âŒ MAUVAIS
- RÃ©sultat : Contexte complet â†’ DÃ©cisions justes â†’ ROI 300%+

**âš ï¸ IMPORTANT** : Toute rÃ©fÃ©rence Ã  "lecture ciblÃ©e" ou "lecture partielle" dans v1.0.0 doit Ãªtre considÃ©rÃ©e comme **OBSOLÃˆTE et DANGEREUSE**.

---

## ğŸ¯ Objectif des Patterns d'Ã‰conomie

L'Ã©conomie de contexte permet de :
1. **PrÃ©server capacitÃ© cognitive** : Tokens disponibles pour raisonnement
2. **Ã‰viter saturation** : PrÃ©venir dÃ©passement limites contexte
3. **Optimiser performance** : RÃ©duire latence et coÃ»ts
4. **Maintenir qualitÃ©** : Concentration sur tÃ¢ches essentielles

**Seuils critiques universels** :
- ğŸŸ¢ 0-30k tokens : Optimal, pleine capacitÃ©
- ğŸŸ¡ 30k-50k tokens : Attention, optimisation recommandÃ©e
- ğŸŸ  50k-100k tokens : Critique, dÃ©lÃ©gation obligatoire
- ğŸ”´ >100k tokens : Maximum, orchestration OBLIGATOIRE

---

## ğŸ—ï¸ Architecture Patterns d'Ã‰conomie

### ğŸ¯ Principe Fondamental : Vraie Ã‰conomie = DÃ©lÃ©gation

**La vraie Ã©conomie de tokens ne vient PAS de la lecture partielle (risquÃ©e), mais de la DÃ‰LÃ‰GATION (sÃ»re).**

- âœ… **DÃ©lÃ©gation** : CrÃ©er sous-tÃ¢ches atomiques via `new_task()` â†’ Chaque sous-tÃ¢che lit ses fichiers COMPLÃˆTEMENT
- âŒ **Fragmentation** : Lire fichiers par morceaux â†’ Angles morts â†’ Re-travail coÃ»teux

**HiÃ©rarchie d'Utilisation** :
1. **Pattern 1 (DÃ©lÃ©gation)** : 80% des cas â†’ Solution privilÃ©giÃ©e universelle
2. **Pattern 2 (DÃ©composition)** : PrÃ©paration dÃ©lÃ©gation â†’ Planification atomique
3. **Pattern 3 (MCP Batch)** : Optimisation opÃ©rations â†’ RÃ©duction overhead
4. **Pattern 4 (Checkpoints)** : Sauvegarde progression â†’ Reprise facilitÃ©e
5. **Pattern 5 (Lecture Intelligente)** : Lecture complÃ¨te obligatoire â†’ Analyses complÃ©mentaires optionnelles
6. **Pattern 6 (Anti-Angles-Morts)** : Principe transversal â†’ Application universelle

### Diagramme HiÃ©rarchie

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â­ PATTERN 1 : DÃ‰LÃ‰GATION INTELLIGENTE (PRIORITAIRE)    â”‚
â”‚ 80% cas â†’ new_task() pour sous-tÃ¢ches atomiques         â”‚
â”‚ Vraie Ã©conomie via parallÃ©lisation, PAS fragmentation   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PATTERN 2 : DÃ‰COMPOSITION ATOMIQUE                      â”‚
â”‚ TÃ¢ches complexes â†’ Sous-tÃ¢ches minimales               â”‚
â”‚ PrÃ©pare dÃ©lÃ©gation efficace                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PATTERN 3 : UTILISATION MCP BATCH                       â”‚
â”‚ OpÃ©rations multiples â†’ RequÃªtes consolidÃ©es            â”‚
â”‚ Optimisation overhead, pas substitut dÃ©lÃ©gation        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PATTERN 4 : CHECKPOINTS SYNTHÃ‰TIQUES                    â”‚
â”‚ Contexte long â†’ SynthÃ¨ses rÃ©guliÃ¨res                   â”‚
â”‚ Sauvegarde progression, facilite reprise               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PATTERN 5 : LECTURE INTELLIGENTE PROGRESSIVE            â”‚
â”‚ Lecture complÃ¨te OBLIGATOIRE â†’ Analyses optionnelles   â”‚
â”‚ Anti-angles-morts : Contexte complet = Non-nÃ©gociable  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ›¡ï¸ PATTERN 6 : ANTI-ANGLES-MORTS (TRANSVERSAL)         â”‚
â”‚ Principe fondamental applicable Ã  TOUS patterns         â”‚
â”‚ Contexte complet AVANT toute dÃ©cision/modification      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“Š RÃ©partition Usage RecommandÃ©e

| Pattern | Usage Typique | Cas Application |
|---------|---------------|-----------------|
| 1. DÃ©lÃ©gation | **80%** | Toute tÃ¢che >30k tokens, fichiers multiples, implÃ©mentation complexe |
| 2. DÃ©composition | **70%** | PrÃ©paration dÃ©lÃ©gation, planning tÃ¢ches complexes |
| 3. MCP Batch | **40%** | Lectures multiples, Ã©ditions coordonnÃ©es, recherches |
| 4. Checkpoints | **30%** | TÃ¢ches >30k tokens, sessions longues |
| 5. Lecture Intelligente | **100%** | TOUT fichier lu (complÃ¨tement d'abord) |
| 6. Anti-Angles-Morts | **100%** | TOUTE dÃ©cision/modification (contexte complet) |

**Note** : Les pourcentages >100% indiquent patterns combinables/transversaux.

---

## 1ï¸âƒ£ PATTERN : DÃ©lÃ©gation Intelligente

### Principe

Le mode **Complex** analyse et conÃ§oit, puis **dÃ©lÃ¨gue l'exÃ©cution** au mode **Simple** via `new_task()`.

### Ã‰conomie Typique

**Sans dÃ©lÃ©gation** :
```markdown
Mode Complex :
1. Analyse architecture (10k tokens)
2. Lecture 15 fichiers (25k tokens)
3. Conception solution (8k tokens)
4. ImplÃ©mentation (12k tokens)
5. Tests (10k tokens)
TOTAL : 65k tokens â†’ SEUIL CRITIQUE DÃ‰PASSÃ‰
```

**Avec dÃ©lÃ©gation** :
```markdown
Mode Complex :
1. Analyse architecture (10k tokens)
2. Conception solution (8k tokens)
3. CrÃ©ation 3 sous-tÃ¢ches (3k tokens)
TOTAL : 21k tokens â†’ OPTIMAL âœ…

Mode Simple (3 instances parallÃ¨les) :
- Sous-tÃ¢che 1 : ImplÃ©mentation Module A (15k tokens)
- Sous-tÃ¢che 2 : ImplÃ©mentation Module B (18k tokens)
- Sous-tÃ¢che 3 : Tests intÃ©gration (12k tokens)
TOTAL : 45k tokens rÃ©partis â†’ OPTIMAL âœ…
```

**Ã‰conomie** : 65k â†’ 21k + (45k/3) = 36k tokens max dans un contexte unique

### Template DÃ©lÃ©gation

```xml
<new_task>
<mode>code-simple</mode>
<message>ğŸ¯ **Sous-tÃ¢che 1.2.1 : ImplÃ©mentation Module Authentification**

**Contexte Architecture (SynthÃ©tique)** :
Architecture JWT validÃ©e avec endpoints /login, /refresh, /logout.
DÃ©pendances approuvÃ©es : bcrypt@5.1.1, jsonwebtoken@9.0.2.

**SpÃ©cification Technique DÃ©taillÃ©e** :
```typescript
// Interface AuthService attendue
interface AuthService {
  login(email: string, password: string): Promise<{token: string, refresh: string}>;
  refreshToken(token: string): Promise<{newToken: string}>;
  logout(token: string): Promise<{success: boolean}>;
}
```

**Fichiers Ã  CrÃ©er** :
- src/services/AuthService.ts (implÃ©mentation complÃ¨te)
- src/tests/AuthService.test.ts (tests unitaires 100% coverage)

**CritÃ¨res Validation** :
- âœ… Toutes mÃ©thodes implÃ©mentÃ©es selon interface
- âœ… Tests unitaires passent (jest)
- âœ… Gestion erreurs complÃ¨te (try/catch)
- âœ… Documentation JSDoc sur chaque mÃ©thode

**Ce qui N'EST PAS dans cette sous-tÃ¢che** :
- IntÃ©gration API routes (sous-tÃ¢che 1.2.2)
- Configuration middleware (sous-tÃ¢che 1.2.3)
- Tests E2E (sous-tÃ¢che 1.3)

**Estimation** : ~15k tokens, 1h30, complexitÃ© modÃ©rÃ©e
</message>
</new_task>
```

**ClÃ©s du succÃ¨s** :
- âœ… Contexte synthÃ©tique (pas de rÃ©pÃ©tition analyse complÃ¨te)
- âœ… SpÃ©cification technique prÃ©cise (pas d'ambiguÃ¯tÃ©)
- âœ… Scope clairement dÃ©limitÃ© (pas de dÃ©rive)
- âœ… CritÃ¨res validation explicites (pas de va-et-vient)

### Cas d'Usage Typiques

**DÃ©lÃ©gation Lecture Batch** :
```xml
<new_task>
<mode>code-simple</mode>
<message>ğŸ¯ **Sous-tÃ¢che 2.1.1 : Extraction Configurations 15 Fichiers**

**Objectif** : Lire et extraire toutes configurations JSON environnements.

**Fichiers** :
- config/env/*.json (10 fichiers)
- config/services/*.json (5 fichiers)

**Format Sortie Attendu** :
```json
{
  "environments": { "dev": {...}, "staging": {...}, "prod": {...} },
  "services": { "api": {...}, "db": {...}, "cache": {...} }
}
```

**Outil** : Utiliser `quickfiles read_multiple_files` pour efficacitÃ©.

**Estimation** : ~8k tokens, 30 min
</message>
</new_task>
```

**Ã‰conomie** : Mode Complex prÃ©serve 8k tokens pour analyse

---

## 2ï¸âƒ£ PATTERN : DÃ©composition Atomique

### Principe

DÃ©composer tÃ¢ches complexes en **sous-tÃ¢ches atomiques** (1 responsabilitÃ©, 1 livrable, 1 validation).

### GranularitÃ© Optimale

**Trop GrossiÃ¨re** (Anti-pattern) :
```markdown
1.1 ImplÃ©menter module utilisateurs (50k tokens)
    â†’ Trop vaste, contexte saturÃ©
```

**Trop Fine** (Anti-pattern) :
```markdown
1.1.1 CrÃ©er interface User (2k tokens)
1.1.2 CrÃ©er classe User (2k tokens)
1.1.3 Ajouter mÃ©thode getEmail (1k tokens)
1.1.4 Ajouter mÃ©thode setEmail (1k tokens)
    â†’ 4 contextes sÃ©parÃ©s, overhead orchestration
```

**Optimale** (Best Practice) :
```markdown
1.1 ImplÃ©menter entitÃ© User complÃ¨te (15k tokens)
    - Interface + Classe + MÃ©thodes + Tests
    - Scope atomique, livrable testable
    âœ… 1 contexte, efficace
```

### RÃ¨gles de DÃ©composition

**CritÃ¨re SMART Atomique** :
- **S**pecific : 1 responsabilitÃ© claire
- **M**easurable : CritÃ¨res validation objectifs
- **A**chievable : RÃ©alisable en <20k tokens
- **R**elevant : Contribue directement objectif parent
- **T**ime-boxed : Estimation durÃ©e <2h

**Seuils recommandÃ©s** :
- Sous-tÃ¢che Simple : 10-20k tokens, 1-2h
- Sous-tÃ¢che ModÃ©rÃ©e : 20-35k tokens, 2-4h
- Sous-tÃ¢che Complexe : 35-50k tokens, 4-6h
- Au-delÃ  50k : DÃ‰COMPOSER ENCORE

### Template DÃ©composition

**TÃ¢che Parent (Analyse)** :
```markdown
## TÃ¢che 1 : SystÃ¨me Authentification Complet

### Analyse ComplexitÃ©
- 5 composants majeurs identifiÃ©s
- Estimation totale : ~120k tokens (CRITIQUE)
- DÃ©composition obligatoire en 5 sous-tÃ¢ches

### Sous-tÃ¢ches Atomiques PlanifiÃ©es
1.1 EntitÃ© User + Repository (20k tokens, 2h)
1.2 Service Authentification JWT (25k tokens, 3h)
1.3 Middleware Validation Token (15k tokens, 1.5h)
1.4 API Routes + Controllers (30k tokens, 3.5h)
1.5 Tests E2E + Documentation (30k tokens, 3h)

TOTAL DÃ‰COMPOSÃ‰ : 120k tokens sur 5 contextes
MAX PAR CONTEXTE : 30k tokens âœ… OPTIMAL
```

**CrÃ©ation Sous-tÃ¢ches** :
```xml
<!-- RÃ©pÃ©ter pour chaque sous-tÃ¢che -->
<new_task>
<mode>code-simple</mode>
<message>ğŸ¯ **Sous-tÃ¢che {{NUMERO}} : {{TITRE}}**

**Contexte HÃ©ritÃ© SynthÃ©tique** :
[DÃ©cisions architecturales clÃ©s, pas d'analyse complÃ¨te]

**Objectif Atomique** :
[1 responsabilitÃ©, 1 livrable]

**DÃ©pendances** :
- Requiert : {{TÃ‚CHES_PRÃ‰ALABLES}} (doivent Ãªtre COMPLÃ‰TÃ‰ES)
- Produit pour : {{TÃ‚CHES_SUIVANTES}}

**Estimation** : {{TOKENS}}k tokens, {{HEURES}}h
</message>
</new_task>
```

---

## 3ï¸âƒ£ PATTERN : Utilisation MCP Batch

### Principe

Utiliser MCPs pour **opÃ©rations batch** au lieu d'outils natifs multiples.

### Ã‰conomie Typique

**Sans MCP Batch** (Anti-pattern) :
```xml
<!-- 5 requÃªtes sÃ©parÃ©es = 5Ã— overhead -->
<read_file><path>module-a.ts</path></read_file>
<read_file><path>module-b.ts</path></read_file>
<read_file><path>module-c.ts</path></read_file>
<read_file><path>module-d.ts</path></read_file>
<read_file><path>module-e.ts</path></read_file>
```

**Avec MCP Batch** (Best Practice) :
```xml
<!-- 1 requÃªte = 1Ã— overhead -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>read_multiple_files</tool_name>
<arguments>
{
  "paths": ["module-a.ts", "module-b.ts", "module-c.ts", "module-d.ts", "module-e.ts"],
  "max_lines_per_file": 500
}
</arguments>
</use_mcp_tool>
```

**Ã‰conomie** : 
- Overhead requÃªtes : 5Ã— â†’ 1Ã— (~80% rÃ©duction)
- Tokens formatting : 5Ã— â†’ 1Ã— (~80% rÃ©duction)
- Temps rÃ©ponse : 5Ã— latence â†’ 1Ã— latence

### Patterns MCP Optimaux

#### Pattern A : Lecture Batch Progressive

```markdown
**Ã‰tape 1** : Explorer structure (minimal)
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>list_directory_contents</tool_name>
<arguments>
{
  "paths": ["src/modules"],
  "recursive": false
}
</arguments>
</use_mcp_tool>

**Ã‰tape 2** : Lire fichiers pertinents identifiÃ©s (batch)
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>read_multiple_files</tool_name>
<arguments>
{
  "paths": ["src/modules/auth.ts", "src/modules/user.ts"],
  "max_lines_per_file": 300
}
</arguments>
</use_mcp_tool>

**Ã‰tape 3** : Lecture approfondie si nÃ©cessaire (ciblÃ©e)
<read_file>
<path>src/modules/auth.ts</path>
<line_range>150-250</line_range>
</read_file>
```

**BÃ©nÃ©fices** :
- Exploration rapide sans surcharge
- Lecture batch des candidats
- Approfondissement ciblÃ© minimal

#### Pattern B : Ã‰dition Batch CoordonnÃ©e

```xml
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>edit_multiple_files</tool_name>
<arguments>
{
  "files": [
    {
      "path": "src/module-a.ts",
      "diffs": [
        {"search": "old import", "replace": "new import", "start_line": 5}
      ]
    },
    {
      "path": "src/module-b.ts",
      "diffs": [
        {"search": "old import", "replace": "new import", "start_line": 8}
      ]
    },
    {
      "path": "src/module-c.ts",
      "diffs": [
        {"search": "old import", "replace": "new import", "start_line": 3}
      ]
    }
  ]
}
</arguments>
</use_mcp_tool>
```

**BÃ©nÃ©fices** :
- AtomicitÃ© (tout ou rien)
- CohÃ©rence garantie
- Ã‰conomie 3Ã— requÃªtes

#### Pattern C : Recherche Multi-Fichiers

```xml
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["src", "tests"],
  "pattern": "function authenticate",
  "use_regex": true,
  "context_lines": 5,
  "max_results_per_file": 3
}
</arguments>
</use_mcp_tool>
```

**BÃ©nÃ©fices** :
- Contexte immÃ©diat (lignes encadrantes)
- Statistiques par fichier
- Limites configurables

---

## 4ï¸âƒ£ PATTERN : Checkpoints SynthÃ©tiques

### Principe

Ã€ intervalles rÃ©guliers, crÃ©er **synthÃ¨ses condensÃ©es** pour libÃ©rer contexte.

### Seuils de Checkpoint

- **30k tokens** : Checkpoint recommandÃ© (prÃ©vention)
- **50k tokens** : Checkpoint OBLIGATOIRE (critique)
- **100k tokens** : Checkpoint + DÃ‰LÃ‰GATION (maximum)

### Format Checkpoint Standard

```markdown
## ğŸ”– Checkpoint 50k Tokens - [Date] [Heure]

### Contexte Actuel
**TÃ¢che** : 1.2 ImplÃ©mentation Service Authentification
**Progression** : 60% (~30k tokens consommÃ©s)
**Prochaine Ã©tape** : Tests unitaires

### Actions Accomplies (SynthÃ¨se)
1. âœ… Interface AuthService dÃ©finie (3 mÃ©thodes)
2. âœ… Classe AuthService implÃ©mentÃ©e (login, refresh, logout)
3. âœ… IntÃ©gration bcrypt + jsonwebtoken validÃ©e
4. âœ… Gestion erreurs complÃ¨te (try/catch)

### DÃ©cisions Techniques ClÃ©s
- **JWT Expiration** : 1h access, 7j refresh (validÃ© utilisateur)
- **Hash Rounds** : bcrypt rounds=12 (sÃ©curitÃ© vs performance)
- **Error Handling** : Exceptions custom AuthError, InvalidTokenError

### Ã‰tat Code Actuel
**Fichiers modifiÃ©s** :
- `src/services/AuthService.ts` : 150 lignes (complet)
- `src/types/auth.types.ts` : 45 lignes (interfaces)

**RÃ©sumÃ© implÃ©mentation** :
```typescript
class AuthService {
  async login(email, password) { /* bcrypt verify + JWT sign */ }
  async refreshToken(token) { /* JWT verify + reissue */ }
  async logout(token) { /* Invalidate token */ }
}
```

### Prochaines Ã‰tapes (PrioritÃ©s)
1. [ ] CrÃ©er AuthService.test.ts (tests unitaires complets)
2. [ ] Valider coverage 100% (jest --coverage)
3. [ ] Documentation JSDoc mÃ©thodes
4. [ ] IntÃ©gration API routes (sous-tÃ¢che 1.2.2)

### Recommandations Ã‰conomie Contexte
**Option A** : Continuer dans contexte actuel (30k restants suffisants pour tests)
**Option B** : CrÃ©er sous-tÃ¢che tests si ajout fonctionnalitÃ©s prÃ©vu

**DÃ©cision** : Option A retenue (tests simples, context OK)
```

**BÃ©nÃ©fices** :
- ğŸ§  MÃ©moire externe projet
- ğŸ”„ Reprise facilitÃ©e aprÃ¨s interruption
- ğŸ“Š MÃ©triques progression prÃ©cises
- ğŸ¯ Focalisation prochaines Ã©tapes

### Template Checkpoint Automatique

```markdown
## ğŸ”– Checkpoint {{TOKENS}}k - {{TIMESTAMP}}

### SynthÃ¨se 3-Points
1. **Fait** : [RÃ©sumÃ© ultra-condensÃ© actions]
2. **DÃ©cidÃ©** : [DÃ©cisions architecturales clÃ©s]
3. **Suivant** : [Prochaines 2-3 Ã©tapes prioritaires]

### Ã‰tat Fichiers (Delta)
- ModifiÃ©s : {{LISTE}} (+{{LIGNES_AJOUTÃ‰ES}}/-{{LIGNES_SUPPRIMÃ‰ES}})
- CrÃ©Ã©s : {{LISTE}}
- Tests : {{STATUT}}

### Recommandation Contexte
{{#if TOKENS > 50}}
ğŸ”´ CRITIQUE : DÃ©lÃ©gation recommandÃ©e
{{else if TOKENS > 30}}
ğŸŸ¡ ATTENTION : Checkpoint effectuÃ©, optimiser suite
{{else}}
ğŸŸ¢ OPTIMAL : Continue dans contexte actuel
{{/if}}
```

---

## 5ï¸âƒ£ PATTERN : Lecture Intelligente Progressive

### âš ï¸ RÃˆGLE D'OR ANTI-ANGLES-MORTS

**Toujours lire un fichier EN ENTIER avant toute analyse partielle.**

Cette rÃ¨gle est **NON-NÃ‰GOCIABLE** pour garantir :
- âœ… ComprÃ©hension contexte global complet
- âœ… Identification de toutes les dÃ©pendances
- âœ… PrÃ©vention des angles morts critiques
- âœ… DÃ©cisions Ã©clairÃ©es basÃ©es sur contexte complet

### Principe Fondamental

La lecture complÃ¨te d'un fichier est **OBLIGATOIRE** comme premiÃ¨re Ã©tape. Les analyses partielles ou ciblÃ©es ne sont que des **complÃ©ments optionnels** APRÃˆS avoir obtenu le contexte global.

**âŒ INTERDIT** : Commencer par des lectures partielles, line_range, ou recherches sans contexte
**âœ… OBLIGATOIRE** : Lecture complÃ¨te d'abord, analyses complÃ©mentaires ensuite SI NÃ‰CESSAIRE

### StratÃ©gie Obligatoire

**Niveau 1 : Lecture ComplÃ¨te (OBLIGATOIRE - Ã‰tape de Grounding)** :
```xml
<!-- TOUJOURS commencer par ceci -->
<read_file>
<path>src/module.ts</path>
</read_file>
```

**Objectif** :
- Grounding complet sur le fichier
- ComprÃ©hension architecture et structure
- Identification zones intÃ©rÃªt pour analyses complÃ©mentaires
- PrÃ©vention angles morts critiques

**RÃ©sultat** : Base solide pour toute dÃ©cision ou modification

**Niveau 2 : Analyses ComplÃ©mentaires (OPTIONNEL - Si NÃ©cessaire)** :

AprÃ¨s avoir lu le fichier complet, vous pouvez SI NÃ‰CESSAIRE :

```xml
<!-- Recherche regex ciblÃ©e pour validation croisÃ©e -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["src/module.ts"],
  "pattern": "function authenticate",
  "context_lines": 3
}
</arguments>
</use_mcp_tool>

<!-- Re-lecture zone spÃ©cifique pour focus -->
<read_file>
<path>src/module.ts</path>
<line_range>150-200</line_range>
</read_file>
```

**Important** : Ces analyses NE REMPLACENT PAS la lecture complÃ¨te initiale, elles la COMPLÃˆTENT.

### Vraie Ã‰conomie : DÃ©lÃ©gation, Pas Fragmentation

**âŒ FAUSSE Ã‰CONOMIE (RisquÃ©e)** :
```markdown
Lire fichier par fragments pour "Ã©conomiser tokens"
â†’ Risque angles morts â†’ Re-travail â†’ CoÃ»t rÃ©el SUPÃ‰RIEUR
```

**âœ… VRAIE Ã‰CONOMIE (SÃ»re)** :
```markdown
Lecture complÃ¨te fichier unique : OK dans contexte actuel
Lecture multiples fichiers : DÃ‰LÃ‰GATION sous-tÃ¢ches atomiques
â†’ Chaque sous-tÃ¢che = 1 fichier lu COMPLÃˆTEMENT
â†’ Ã‰conomie via parallÃ©lisation, pas fragmentation risquÃ©e
```

**Exemple DÃ©lÃ©gation Optimale** :
```xml
<new_task>
<mode>code-simple</mode>
<message>ğŸ¯ **Sous-tÃ¢che 3.1 : Analyse Module Authentification**

**Objectif** : Lire et analyser COMPLÃˆTEMENT le module d'authentification.

**Fichier** : src/modules/auth.ts (lecture INTÃ‰GRALE requise)

**Livrable** :
- SynthÃ¨se architecture du module
- Liste dÃ©pendances identifiÃ©es
- Points attention pour intÃ©gration

**Note** : Lecture complÃ¨te OBLIGATOIRE pour Ã©viter angles morts.
</message>
</new_task>
```

### Pattern Markdown Structure (AprÃ¨s Lecture ComplÃ¨te)

Pour documentation volumineuse, la structure peut aider APRÃˆS lecture complÃ¨te :

```xml
<!-- Ã‰tape 1 : TOUJOURS lire document complet d'abord -->
<read_file>
<path>docs/architecture.md</path>
</read_file>

<!-- Ã‰tape 2 (optionnel) : Extraire structure pour navigation -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>extract_markdown_structure</tool_name>
<arguments>
{
  "paths": ["docs/architecture.md"],
  "max_depth": 3,
  "include_context": false
}
</arguments>
</use_mcp_tool>
```

**Ordre CRITIQUE** : Lecture complÃ¨te AVANT structure, jamais l'inverse

---

## ğŸ“Š Matrice DÃ©cision Ã‰conomie

### Flowchart Optimisation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokens actuels? â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ <30k?  â”‚
    â””â”€â”¬â”€â”€â”€â”€â”¬â”€â”˜
      â”‚    â”‚
     OUI  NON
      â”‚    â”‚
      â†“    â†“
   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ OK  â”‚ â”‚ 30-50k?  â”‚
   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”˜
             â”‚      â”‚
            OUI    NON
             â”‚      â”‚
             â†“      â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚CHECKPOINTâ”‚ â”‚ >50k?    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”˜
                     â”‚      â”‚
                    OUI    NON
                     â”‚      â”‚
                     â†“      â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ DÃ‰LÃ‰GUER  â”‚ â”‚ >100k?   â”‚
              â”‚ new_task()â”‚ â””â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”˜
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚
                             OUI    NON
                              â”‚      â”‚
                              â†“      â†“
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ORCHESTR. â”‚ â”‚CHECKPOINTâ”‚
                       â”‚OBLIGAT.  â”‚ â”‚+ DÃ‰LÃ‰GAT.â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6ï¸âƒ£ PATTERN : Anti-Angles-Morts (PrÃ©vention Blind Spots)

### ğŸ¯ Principe Fondamental

**Les angles morts sont l'ennemi silencieux de la qualitÃ© logicielle.**

Un "angle mort" survient lorsqu'une dÃ©cision ou modification est prise sur la base d'un contexte **incomplet** ou **fragmentÃ©**, conduisant Ã  :
- ğŸ› Bugs subtils non dÃ©tectÃ©s
- ğŸ”„ Re-travail coÃ»teux
- ğŸ’¥ RÃ©gression fonctionnelle
- ğŸ§© IncohÃ©rences architecturales
- â±ï¸ CoÃ»t total SUPÃ‰RIEUR aux tokens "Ã©conomisÃ©s"

### âš ï¸ RÃ¨gle Anti-Angles-Morts

```
AVANT toute dÃ©cision ou modification :
Contexte COMPLET = OBLIGATOIRE

Fichier = UnitÃ© atomique d'information
â†’ TOUJOURS lire EN ENTIER
```

### Workflow Obligatoire Anti-Angles-Morts

**Ã‰tape 1 : Identification Fichiers Pertinents** :
```xml
<!-- Recherche sÃ©mantique pour dÃ©couverte -->
<codebase_search>
<query>authentication jwt token validation</query>
<path>src</path>
</codebase_search>

<!-- Ou exploration structure -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>list_directory_contents</tool_name>
<arguments>{"paths": ["src/auth"], "recursive": true}</arguments>
</use_mcp_tool>
```

**Ã‰tape 2 : Lecture ComplÃ¨te CHAQUE Fichier IdentifiÃ©** :
```xml
<!-- âœ… CORRECT : Lecture intÃ©grale -->
<read_file>
<path>src/auth/AuthService.ts</path>
</read_file>

<read_file>
<path>src/auth/TokenValidator.ts</path>
</read_file>
```

**Ã‰tape 3 : SynthÃ¨se Mentale Contexte Global** :
```markdown
AprÃ¨s lectures complÃ¨tes, mental model :
- Architecture AuthService utilise TokenValidator
- DÃ©pendance jwt library version 9.0.2
- Validation tokens inclut refresh + access
- Error handling via custom AuthError
â†’ Base solide pour modifications
```

**Ã‰tape 4 : Analyses ComplÃ©mentaires (Si NÃ©cessaire)** :
```xml
<!-- Validation croisÃ©e APRÃˆS contexte complet -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["src"],
  "pattern": "AuthError",
  "context_lines": 5
}
</arguments>
</use_mcp_tool>
```

**Ã‰tape 5 : DÃ©cisions Ã‰clairÃ©es** :
```markdown
DÃ©cision basÃ©e sur :
âœ… Architecture complÃ¨te comprise
âœ… DÃ©pendances identifiÃ©es
âœ… Patterns existants respectÃ©s
âœ… Impacts anticipÃ©s

â†’ Modifications cohÃ©rentes et sÃ»res
```

### ğŸš« Anti-Patterns Flagrants

#### âŒ Anti-Pattern 1 : Search-First (Recherche Sans Contexte)

**MAUVAIS** :
```xml
<!-- Chercher fonction sans lire fichier complet -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["src/auth/AuthService.ts"],
  "pattern": "function login"
}
</arguments>
</use_mcp_tool>

<!-- Modifier basÃ© sur fragment trouvÃ© -->
<apply_diff>
<path>src/auth/AuthService.ts</path>
<diff>...</diff>
</apply_diff>
```

**ProblÃ¨mes** :
- ğŸ”´ Architecture globale ignorÃ©e
- ğŸ”´ DÃ©pendances manquÃ©es
- ğŸ”´ Side-effects non anticipÃ©s
- ğŸ”´ Risque breaking changes

**CORRECT** :
```xml
<!-- 1. Lire fichier COMPLET d'abord -->
<read_file>
<path>src/auth/AuthService.ts</path>
</read_file>

<!-- 2. Comprendre architecture globale -->
<!-- 3. PUIS modifier en connaissance de cause -->
<apply_diff>
<path>src/auth/AuthService.ts</path>
<diff>...</diff>
</apply_diff>
```

#### âŒ Anti-Pattern 2 : Line-Range-First (Lecture Partielle)

**MAUVAIS** :
```xml
<!-- Lire uniquement lignes "intÃ©ressantes" -->
<read_file>
<path>src/config/database.ts</path>
<line_range>50-100</line_range>
</read_file>

<!-- Manque : imports, constantes globales, exports -->
```

**ProblÃ¨mes** :
- ğŸ”´ Imports manquÃ©s (dÃ©pendances inconnues)
- ğŸ”´ Constantes globales ignorÃ©es
- ğŸ”´ Exports incompris
- ğŸ”´ DÃ©cision basÃ©e sur vision tronquÃ©e

**CORRECT** :
```xml
<!-- Lire fichier COMPLET -->
<read_file>
<path>src/config/database.ts</path>
</read_file>

<!-- Optionnel : Re-focus zone si besoin APRÃˆS -->
<read_file>
<path>src/config/database.ts</path>
<line_range>50-100</line_range>
</read_file>
```

#### âŒ Anti-Pattern 3 : Assumption-Driven (HypothÃ¨ses Non VÃ©rifiÃ©es)

**MAUVAIS** :
```markdown
"Je suppose que cette fonction fait X"
â†’ Modification basÃ©e sur supposition
â†’ Sans vÃ©rifier contexte complet
```

**ProblÃ¨mes** :
- ğŸ”´ HypothÃ¨ses souvent fausses
- ğŸ”´ Comportements surprenants ignorÃ©s
- ğŸ”´ Edge cases manquÃ©s
- ğŸ”´ Tests cassÃ©s

**CORRECT** :
```markdown
1. Lire fichier complet
2. Comprendre RÃ‰ELLEMENT ce que fait fonction
3. VÃ©rifier edge cases et error handling
4. PUIS modifier en connaissance complÃ¨te
```

#### âŒ Anti-Pattern 4 : Quick-Fix-Syndrome (Modification Rapide)

**MAUVAIS** :
```markdown
"Je vais juste modifier cette ligne rapidement"
â†’ Sans lire reste du fichier
â†’ Sans comprendre contexte global
```

**ProblÃ¨mes** :
- ğŸ”´ RÃ©gression introduite
- ğŸ”´ IncohÃ©rences avec reste code
- ğŸ”´ Breaking changes non dÃ©tectÃ©s
- ğŸ”´ CoÃ»t re-travail > temps "gagnÃ©"

**CORRECT** :
```markdown
1. MÃªme pour "petite" modification : Lire fichier complet
2. Comprendre impact potentiel
3. VÃ©rifier cohÃ©rence avec architecture
4. Modifier en sÃ©curitÃ©
```

### âœ… Exceptions Rares (Usage AvancÃ©)

**Exception 1 : Fichiers Volumineux >10k Lignes** :
```xml
<!-- OK : Lecture complÃ¨te + focus zones -->
<read_file>
<path>src/generated/api-client.ts</path>
</read_file>

<!-- 15000 lignes lues, focus sur zone modifiable -->
<read_file>
<path>src/generated/api-client.ts</path>
<line_range>8000-8500</line_range>
</read_file>
```

**Justification** : Contexte global obtenu, re-lecture zone pour clartÃ©

**Exception 2 : Logs Volumineux** :
```xml
<!-- OK : StratÃ©gie logs dÃ©finie AVANT recherche -->
<read_file>
<path>logs/app.log</path>
<line_range>-1000</line_range> <!-- 1000 derniÃ¨res lignes -->
</read_file>

<!-- Recherche patterns erreurs -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["logs/app.log"],
  "pattern": "ERROR|FATAL",
  "context_lines": 10
}
</arguments>
</use_mcp_tool>
```

**Justification** : Logs = nature diffÃ©rente (sÃ©quentiel, non-architectural)

**Exception 3 : Data Files (CSV, JSON volumineux)** :
```xml
<!-- OK : Headers + Ã©chantillon PUIS recherche -->
<read_file>
<path>data/users.csv</path>
<line_range>1-100</line_range> <!-- Headers + premiers records -->
</read_file>

<!-- Recherche record spÃ©cifique si besoin -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["data/users.csv"],
  "pattern": "user-12345"
}
</arguments>
</use_mcp_tool>
```

**Justification** : Data tabulaire = structure rÃ©pÃ©titive, Ã©chantillon suffit

### ğŸ“š Cas d'Ã‰cole : Incident README.md

**Contexte RÃ©el** :
```markdown
Utilisateur demande : "Analyse le README.md du projet"

âŒ MA PREMIÃˆRE TENTATIVE (MAUVAISE) :
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["README.md"],
  "pattern": "installation|configuration|usage"
}
</arguments>
</use_mcp_tool>

ğŸ”´ FEEDBACK UTILISATEUR :
"Lis le fichier EN ENTIER pour ne pas avoir d'angles morts"
```

**ProblÃ¨me IdentifiÃ©** :
- Recherche fragmentÃ©e a ratÃ© sections importantes
- Vision tronquÃ©e du projet
- Recommandations basÃ©es sur contexte incomplet
- Angles morts sur architecture et dÃ©pendances

**Correction** :
```xml
<read_file>
<path>README.md</path>
</read_file>
```

**RÃ©sultat** :
- âœ… Contexte complet obtenu
- âœ… Toutes sections comprises (mÃªme non searchÃ©es)
- âœ… Recommandations pertinentes et cohÃ©rentes
- âœ… ZÃ©ro angle mort

**LeÃ§on** : **1 fichier = 1 lecture complÃ¨te, TOUJOURS**

### ğŸ’° CoÃ»t RÃ©el : Angles Morts vs Tokens "Ã‰conomisÃ©s"

**ScÃ©nario Typique** :

```markdown
### Sans Anti-Angles-Morts (Lecture Partielle)
1. Lecture partielle fichier (line_range) : 2k tokens Ã©conomisÃ©s
2. Modification basÃ©e sur contexte incomplet
3. Tests Ã©chouent â†’ Re-lecture complÃ¨te : 5k tokens
4. DÃ©couverte dÃ©pendance manquÃ©e â†’ Re-lecture autre fichier : 3k tokens
5. Correction modification : 2k tokens
6. Re-tests : 1k tokens

TOTAL RÃ‰EL : 13k tokens + 3h de re-travail
Ã‰CONOMIE INITIALE : 2k tokens
COÃ›T NET : +11k tokens + 3h perdues âŒ

### Avec Anti-Angles-Morts (Lecture ComplÃ¨te)
1. Lecture complÃ¨te fichier d'emblÃ©e : 5k tokens
2. Contexte complet = dÃ©cision Ã©clairÃ©e
3. Modification correcte premier coup : 2k tokens
4. Tests passent : 1k tokens

TOTAL : 8k tokens + 1h travail efficace
Ã‰CONOMIE NETTE : 5k tokens + 2h gagnÃ©es âœ…
```

**Conclusion MathÃ©matique** :
```
Lecture complÃ¨te d'emblÃ©e = MOINS coÃ»teuse que lecture partielle + re-travail

Angles morts = Fausse Ã©conomie
Contexte complet = Vraie Ã©conomie
```

### ğŸ¯ Impact sur QualitÃ© DÃ©cisions

**DÃ©cisions avec Angles Morts** :
- ğŸ”´ BasÃ©es sur hypothÃ¨ses
- ğŸ”´ Ignorent cas edge
- ğŸ”´ Ratent dÃ©pendances critiques
- ğŸ”´ IncohÃ©rentes avec architecture existante
- ğŸ”´ NÃ©cessitent corrections multiples

**DÃ©cisions avec Contexte Complet** :
- âœ… BasÃ©es sur faits vÃ©rifiÃ©s
- âœ… Anticipent cas edge
- âœ… Respectent dÃ©pendances
- âœ… CohÃ©rentes avec architecture
- âœ… Correctes premier coup

**MÃ©triques QualitÃ©** :
```
Taux succÃ¨s premier coup :
- Avec angles morts : ~40%
- Avec contexte complet : ~85%

Temps moyen rÃ©solution :
- Avec angles morts : 3-5h (itÃ©rations multiples)
- Avec contexte complet : 1-2h (direct)
```

### ğŸ›¡ï¸ RÃ©sumÃ© ExÃ©cutif

**3 RÃ¨gles d'Or** :
1. **Fichier = UnitÃ© Atomique** : Toujours lire en entier
2. **Contexte Avant Action** : Comprendre avant modifier
3. **DÃ©lÃ©gation Si Multiple** : N fichiers = N sous-tÃ¢ches (chacune lit 1 fichier complet)

**Formule SuccÃ¨s** :
```
QualitÃ© Code = f(Contexte Complet)
Contexte Complet â‡’ Lecture IntÃ©grale Fichiers
âˆ´ Lecture IntÃ©grale = Non-nÃ©gociable
```

**Rappel** : Les tokens "Ã©conomisÃ©s" par lectures partielles sont une **illusion** si le coÃ»t du re-travail est pris en compte. La vraie Ã©conomie passe par la **dÃ©lÃ©gation**, pas la fragmentation.


### Tableau Recommandations

| Tokens | Ã‰tat | Action ImmÃ©diate | Pattern RecommandÃ© |
|--------|------|------------------|-------------------|
| 0-10k | ğŸŸ¢ DÃ©marrage | Aucune | Exploration normale |
| 10-30k | ğŸŸ¢ Optimal | Aucune | Continue, MCP batch si opportun |
| 30-40k | ğŸŸ¡ Attention | Checkpoint prÃ©ventif | Patterns 3-5 (Batch, Lecture ciblÃ©e) |
| 40-50k | ğŸŸ¡ Vigilance | Checkpoint + Analyse | Patterns 1-2 (DÃ©lÃ©gation prÃ©parÃ©e) |
| 50-70k | ğŸŸ  Critique | DÃ©lÃ©gation OBLIGATOIRE | Pattern 1-2 (new_task atomiques) |
| 70-100k | ğŸŸ  Maximum | DÃ©lÃ©gation + Checkpoint | Pattern 2 + 4 (DÃ©composition + SynthÃ¨se) |
| >100k | ğŸ”´ DÃ©passement | Orchestration IMMÃ‰DIATE | Escalade orchestrator-complex |

---

## ğŸ¨ Templates Instructions Modes

### Template Mode Simple

```markdown
## Ã‰CONOMIE CONTEXTE

### Monitoring Tokens
Ã€ chaque outil use, Ã©valuer :
- Tokens consommÃ©s approximatifs
- Total cumulÃ© actuel
- Distance seuil suivant (30k, 50k)

### Actions PrÃ©ventives
**Si approche 30k** :
- Checkpoint synthÃ©tique
- Prioriser tÃ¢ches restantes
- Ã‰valuer possibilitÃ© finalisation

**Si approche 50k** :
- Checkpoint OBLIGATOIRE
- Escalade vers mode complexe si tÃ¢che incomplÃ¨te
- Ou finalisation rapide si quasi-terminÃ©

### Utilisation MCP Efficace
- PrivilÃ©gier `quickfiles` pour opÃ©rations batch
- Lecture ciblÃ©e (line_range) au lieu de complÃ¨te
- Ã‰viter rÃ©pÃ©titions requÃªtes similaires
```

### Template Mode Complex

```markdown
## Ã‰CONOMIE CONTEXTE AVANCÃ‰E

### Patterns DÃ©lÃ©gation
**DÃ¨s 30k tokens** : Ã‰valuer dÃ©composition
**DÃ¨s 50k tokens** : DÃ©lÃ©gation OBLIGATOIRE

### CrÃ©ation Sous-tÃ¢ches Atomiques
```xml
<new_task>
<mode>code-simple</mode>
<message>ğŸ¯ **Sous-tÃ¢che {{NUMERO}} : {{TITRE}}**

**Contexte SynthÃ©tique** : [Essentiel seulement]
**Objectif Atomique** : [1 responsabilitÃ©]
**Estimation** : {{TOKENS}}k tokens, {{HEURES}}h
</message>
</new_task>
```

### Checkpoints RÃ©guliers
- 30k : Checkpoint prÃ©ventif
- 50k : Checkpoint + DÃ©lÃ©gation Ã©valuÃ©e
- 70k : Checkpoint + DÃ©lÃ©gation OBLIGATOIRE

### StratÃ©gie Analyse puis DÃ©lÃ©gation
1. Analyser et concevoir (garder contexte riche)
2. CrÃ©er sous-tÃ¢ches implÃ©mentation (contexte synthÃ©tique)
3. Modes simples exÃ©cutent (contextes lÃ©gers)
4. Valider et intÃ©grer (contexte prÃ©servÃ©)
```

---

## âš ï¸ Anti-Patterns Ã  Ã‰viter

### âœ… Lecture ComplÃ¨te d'Abord (Grounding Obligatoire)
```xml
<!-- BON : Lecture intÃ©grale pour contexte complet -->
<read_file>
<path>module.ts</path>
</read_file>

<!-- Puis analyses complÃ©mentaires SI NÃ‰CESSAIRE -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["module.ts"],
  "pattern": "function authenticate",
  "context_lines": 3
}
</arguments>
</use_mcp_tool>
```

**Justification** :
- âœ… Contexte global complet obtenu
- âœ… PrÃ©vention angles morts
- âœ… DÃ©cisions Ã©clairÃ©es
- âœ… ZÃ©ro risque information manquÃ©e

### âŒ Lecture FragmentÃ©e Sans Contexte Global
```xml
<!-- MAUVAIS : Recherche ou line_range SANS lecture complÃ¨te prÃ©alable -->
<use_mcp_tool>
<server_name>quickfiles</server_name>
<tool_name>search_in_files</tool_name>
<arguments>
{
  "paths": ["module.ts"],
  "pattern": "function authenticate"
}
</arguments>
</use_mcp_tool>

<!-- Ou pire : Lecture partielle directe -->
<read_file>
<path>module.ts</path>
<line_range>200-250</line_range>
</read_file>

<!-- Sans JAMAIS avoir lu le fichier complet -->
```

**ProblÃ¨mes** :
- ğŸ”´ Angles morts critiques (informations manquÃ©es)
- ğŸ”´ DÃ©cisions basÃ©es sur contexte incomplet
- ğŸ”´ Risque bugs subtils et rÃ©gressions
- ğŸ”´ CoÃ»t re-travail > tokens "Ã©conomisÃ©s"

**Note Importante** : `list_code_definition_names` est Ã©galement un anti-pattern si utilisÃ© SANS lecture complÃ¨te. Il ne remplace PAS la lecture intÃ©grale, il peut seulement la COMPLÃ‰TER.

### âŒ Pas de Checkpoint Avant Saturation
```markdown
Mode continue jusqu'Ã  95k tokens sans checkpoint
â†’ Pas de point reprise, contexte perdu si erreur
```

### âœ… Checkpoints RÃ©guliers PrÃ©ventifs
```markdown
- 30k : Checkpoint 1 (prÃ©vention)
- 50k : Checkpoint 2 (critique)
- 70k : Checkpoint 3 + dÃ©lÃ©gation
â†’ Points reprise multiples, traÃ§abilitÃ© complÃ¨te
```

### âŒ DÃ©composition Excessive
```markdown
DÃ©composition en 50 micro-tÃ¢ches de 2k tokens chacune
â†’ Overhead orchestration Ã©norme, perte vue ensemble
```

### âœ… DÃ©composition Ã‰quilibrÃ©e
```markdown
DÃ©composition en 5-8 sous-tÃ¢ches atomiques 15-30k chacune
â†’ GranularitÃ© optimale, orchestration efficace
```

---

## ğŸ“Š MÃ©triques Ã‰conomie Contexte

### âš ï¸ Principe MÃ©trique : CoÃ»t Total RÃ©el

**Les mÃ©triques doivent inclure le COÃ›T COMPLET**, pas seulement tokens consommÃ©s :

```
CoÃ»t Total RÃ©el = Tokens Lecture + Tokens Re-travail + CoÃ»t Bugs/RÃ©gressions

Lecture Partielle : Tokens initiaux faibles MAIS re-travail Ã©levÃ©
Lecture ComplÃ¨te : Tokens initiaux moyens MAIS re-travail quasi-nul

âˆ´ Lecture ComplÃ¨te = CoÃ»t Total INFÃ‰RIEUR
```

### Par TÃ¢che (MÃ©triques Enrichies)

```markdown
## Rapport Ã‰conomie Contexte - TÃ¢che {{ID}}

### Utilisation Tokens
- Tokens consommÃ©s : {{CONSUMED}}k
- Tokens Ã©conomisÃ©s (dÃ©lÃ©gation) : {{SAVED_DELEGATION}}k
- Tokens Ã©conomisÃ©s (MCP batch) : {{SAVED_BATCH}}k
- **CoÃ»t angles morts Ã©vitÃ©s** : {{SAVED_BLIND_SPOTS}}k
- EfficacitÃ© nette : {{NET_EFFICIENCY}}%

### Patterns AppliquÃ©s (SÃ»rs)
- âœ… DÃ©lÃ©gation intelligente : {{COUNT_DELEGATION}} sous-tÃ¢ches
- âœ… MCP Batch : {{COUNT_BATCH}} vs {{COUNT_NATIVE}} natifs
- âœ… Lectures complÃ¨tes : {{COUNT_FULL_READ}} fichiers

### Patterns RisquÃ©s Ã‰vitÃ©s
- âŒ Lectures partielles Ã©vitÃ©es : {{COUNT_AVOIDED_PARTIAL}}
- âŒ Recherches sans contexte Ã©vitÃ©es : {{COUNT_AVOIDED_SEARCH_FIRST}}
- ğŸ›¡ï¸ Angles morts prÃ©venus : {{COUNT_BLIND_SPOTS_PREVENTED}}

### Impact QualitÃ©
- Modifications correctes 1er coup : {{FIRST_TIME_RIGHT}}%
- Re-travail nÃ©cessaire : {{REWORK_COUNT}} itÃ©rations
- Bugs dÃ©tectÃ©s post-livraison : {{BUGS_POST_DELIVERY}}
- Temps total (incluant corrections) : {{TOTAL_TIME}}h

### Seuils Franchis
- 30k : {{TIMESTAMP_30K}} (checkpoint prÃ©ventif)
- 50k : {{TIMESTAMP_50K}} (+ checkpoint + Ã©valuation dÃ©lÃ©gation)
- 70k : {{TIMESTAMP_70K}} (+ dÃ©lÃ©gation OBLIGATOIRE)

### Comparaison ScÃ©narios

| ScÃ©nario | Tokens Initiaux | Re-travail | Total | QualitÃ© |
|----------|-----------------|------------|-------|---------|
| **Lecture Partielle** | {{PARTIAL_INITIAL}}k | {{PARTIAL_REWORK}}k | {{PARTIAL_TOTAL}}k | {{PARTIAL_QUALITY}}% |
| **Lecture ComplÃ¨te** | {{FULL_INITIAL}}k | {{FULL_REWORK}}k | {{FULL_TOTAL}}k | {{FULL_QUALITY}}% |
| **Ã‰conomie RÃ©elle** | +{{DIFF_INITIAL}}k | -{{DIFF_REWORK}}k | **-{{DIFF_TOTAL}}k** âœ… | +{{DIFF_QUALITY}}% |

### Recommandations Futures
- Patterns sÃ»rs qui ont bien fonctionnÃ© : [DÃ©lÃ©gation, Lecture complÃ¨te, MCP batch]
- Patterns risquÃ©s Ã©vitÃ©s avec succÃ¨s : [Lecture partielle, Search-first]
- AmÃ©lioration continue : [Points spÃ©cifiques identifiÃ©s]
```

### Par Mode (AgrÃ©gÃ©)

```markdown
## Rapport Mode {{MODE_SLUG}} - PÃ©riode {{PERIOD}}

### Statistiques Tokens
- Moyenne par tÃ¢che : {{AVG_TOKENS}}k
- Maximum atteint : {{MAX_TOKENS}}k
- TÃ¢ches >50k : {{COUNT_CRITICAL}} ({{PERCENT}}%)

### EfficacitÃ© Patterns SÃ»rs
- DÃ©lÃ©gations effectuÃ©es : {{COUNT_DELEGATIONS}}
- Ã‰conomie moyenne dÃ©lÃ©gation : {{AVG_SAVED_DELEGATION}}k tokens/dÃ©lÃ©gation
- Taux MCP batch : {{MCP_RATE}}%
- Taux lecture complÃ¨te : {{FULL_READ_RATE}}% (objectif: 100%)

### PrÃ©vention Angles Morts
- Lectures complÃ¨tes vs partielles : {{FULL_VS_PARTIAL_RATIO}}
- Angles morts Ã©vitÃ©s : {{BLIND_SPOTS_AVOIDED}}
- Ã‰conomie re-travail : {{REWORK_SAVED}}k tokens
- CoÃ»t bugs prÃ©venus : {{BUGS_PREVENTED_COST}}k tokens Ã©quivalent

### QualitÃ© & FiabilitÃ©
- TÃ¢ches complÃ©tÃ©es sans saturation : {{SUCCESS_RATE}}%
- **TÃ¢ches correctes 1er coup** : {{FIRST_TIME_RIGHT_RATE}}%
- Re-travail moyen : {{AVG_REWORK}} itÃ©rations/tÃ¢che
- Escalades forcÃ©es (>100k) : {{ESCALATIONS}}
- Bugs post-livraison : {{POST_DELIVERY_BUGS}}

### ROI Patterns

| Pattern | CoÃ»t ImplÃ©mentation | Ã‰conomie GÃ©nÃ©rÃ©e | ROI | Risque |
|---------|---------------------|------------------|-----|--------|
| DÃ©lÃ©gation | Faible (planning) | **TrÃ¨s Ã©levÃ©e** (parallÃ©lisation) | **500%** | âœ… Nul |
| Lecture ComplÃ¨te | Moyen (tokens) | **Ã‰levÃ©e** (Ã©vite re-travail) | **300%** | âœ… Nul |
| MCP Batch | Faible (setup) | Moyenne (overhead) | 200% | âœ… Nul |
| Lecture Partielle | Faible (tokens) | **NÃ©gative** (re-travail) | **-150%** | ğŸ”´ Ã‰levÃ© |

**Conclusion** : Patterns "Ã©conomes en tokens" peuvent avoir ROI **nÃ©gatif** si re-travail pris en compte.
```

---

## ğŸš€ BÃ©nÃ©fices Ã‰conomie Contexte (RÃ©visÃ©s)

### BÃ©nÃ©fices Quantifiables

1. **Performance Tokens** : -40% tokens moyens par tÃ¢che (via dÃ©lÃ©gation)
2. **QualitÃ© Code** : +85% modifications correctes 1er coup (via lecture complÃ¨te)
3. **PrÃ©vention Angles Morts** : -90% bugs subtils (via contexte complet)
4. **RÃ©duction Re-travail** : -75% itÃ©rations corrections (via dÃ©cisions Ã©clairÃ©es)
5. **CoÃ»ts API** : -35% coÃ»ts globaux (dÃ©lÃ©gation + prÃ©vention re-travail)
6. **FiabilitÃ©** : +80% tÃ¢ches finalisÃ©es premier essai
7. **MaintenabilitÃ©** : Checkpoints facilitent reprises (+60% vitesse reprise)

### Comparaison Approches

**Approche "Fausse Ã‰conomie" (Lecture Partielle)** :
```
Tokens lecture : 2k (Ã©conomie apparente)
Tokens re-travail : 11k (angles morts)
Temps perdu : 3h
Bugs introduits : 2-3
TOTAL : 13k tokens + 3h + bugs âŒ
```

**Approche "Vraie Ã‰conomie" (Lecture ComplÃ¨te + DÃ©lÃ©gation)** :
```
Tokens lecture complÃ¨te : 5k
Tokens re-travail : 1k (dÃ©cisions justes)
Temps Ã©conomisÃ© : 2h
Bugs introduits : 0
TOTAL : 6k tokens + gain 2h + qualitÃ© âœ…
```

**Gain Net** : 7k tokens + 5h + Ã©limination bugs = **ROI 300%+**

### Impact Long Terme

- **Confiance Utilisateur** : DÃ©cisions basÃ©es contexte complet â†’ Recommandations fiables
- **VÃ©locitÃ© Projet** : Moins corrections â†’ Progression linÃ©aire stable
- **Dette Technique** : PrÃ©vention angles morts â†’ Code maintenable long terme
- **Apprentissage** : Patterns sÃ»rs rÃ©utilisables â†’ AmÃ©lioration continue

---

## ğŸ“š Ressources ComplÃ©mentaires

- [`escalade-mechanisms-revised.md`](escalade-mechanisms-revised.md) : DÃ©lÃ©gation et dÃ©sescalade Ã©conomique
- [`hierarchie-numerotee-subtasks.md`](hierarchie-numerotee-subtasks.md) : DÃ©composition atomique optimale
- [`sddd-protocol-4-niveaux.md`](sddd-protocol-4-niveaux.md) : Grounding initial complet obligatoire
- [`mcp-integrations-priority.md`](mcp-integrations-priority.md) : Utilisation MCP batch efficace
- [`factorisation-commons.md`](factorisation-commons.md) : RÃ©duction instructions communes

---

## ğŸ”— CohÃ©rence Architecturale

### Alignement avec SDDD Protocol (4 Niveaux)

Le protocole SDDD impose un **Grounding Initial** obligatoire au dÃ©but de chaque tÃ¢che. Cette rÃ©vision du document [`context-economy-patterns.md`](context-economy-patterns.md) renforce cette exigence :

- **SDDD Niveau 1 (Grounding Initial)** â†”ï¸ **Pattern 6 (Anti-Angles-Morts)** : Contexte complet AVANT action
- **SDDD Niveau 2 (Checkpoint Milieu)** â†”ï¸ **Pattern 4 (Checkpoints)** : SynthÃ¨ses rÃ©guliÃ¨res
- **SDDD Niveau 3 (Validation Finale)** â†”ï¸ **MÃ©triques QualitÃ©** : VÃ©rification correctness 1er coup

**CohÃ©rence ValidÃ©e** : âœ… Le grounding complet (lecture intÃ©grale fichiers) est maintenant **NON-NÃ‰GOCIABLE** dans les deux documents.

### Alignement avec HiÃ©rarchie Sous-TÃ¢ches

Le document [`hierarchie-numerotee-subtasks.md`](hierarchie-numerotee-subtasks.md) dÃ©finit `new_task()` comme mÃ©canisme universel de dÃ©lÃ©gation. Cette rÃ©vision positionne la **DÃ©lÃ©gation comme Pattern #1 prioritaire (80% des cas)** :

- **DÃ©composition Atomique** â†”ï¸ **Pattern 2** : Sous-tÃ¢ches 15-30k tokens
- **new_task() Universel** â†”ï¸ **Pattern 1 DÃ©lÃ©gation** : MÃ©canisme privilÃ©giÃ©
- **Scope DÃ©limitÃ©** â†”ï¸ **Lecture ComplÃ¨te** : Chaque sous-tÃ¢che lit ses fichiers EN ENTIER

**CohÃ©rence ValidÃ©e** : âœ… La dÃ©lÃ©gation via `new_task()` est le pattern #1, chaque sous-tÃ¢che applique principe anti-angles-morts.

### Alignement avec Escalade Mechanisms

Le document [`escalade-mechanisms-revised.md`](escalade-mechanisms-revised.md) clarifie la distinction entre dÃ©lÃ©gation (prÃ©fÃ©rÃ©e) et escalade (dernier recours). Cette rÃ©vision renforce cette distinction :

- **DÃ©lÃ©gation PrÃ©ventive** â†”ï¸ **Pattern 1** : Ã‰vite saturation contexte
- **Escalade ForcÃ©e** â†”ï¸ **Seuil >100k** : Dernier recours seulement
- **DÃ©composition vs Escalade** â†”ï¸ **Patterns 1-2 vs Switch Mode** : DÃ©lÃ©gation privilÃ©giÃ©e

**CohÃ©rence ValidÃ©e** : âœ… DÃ©lÃ©gation = pattern prioritaire, escalade = exception rare.

### Principe Transversal UnifiÃ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          RÃˆGLE D'OR TRANSVERSALE (Tous Documents)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Contexte Complet AVANT DÃ©cision/Action                    â”‚
â”‚                                                             â”‚
â”‚  - SDDD Protocol : Grounding initial obligatoire           â”‚
â”‚  - Context Economy : Lecture intÃ©grale fichiers            â”‚
â”‚  - HiÃ©rarchie Subtasks : Scope complet par sous-tÃ¢che      â”‚
â”‚  - Escalade Mechanisms : Contexte avant dÃ©lÃ©gation         â”‚
â”‚                                                             â”‚
â”‚  âš ï¸ Angles Morts = Ennemi Commun PrÃ©venu par Tous Patterns â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Note Finale RÃ©visÃ©e

**Version 2.0.0 - RÃ©vision Majeure FB-04**

L'Ã©conomie de contexte n'est **pas une contrainte** mais une **opportunitÃ© d'optimisation sÃ»re**. Cette rÃ©vision majeure clarifie un principe fondamental :

### ğŸ¯ Message ClÃ©

**La VRAIE Ã©conomie de tokens = DÃ‰LÃ‰GATION, pas lecture partielle risquÃ©e.**

### âš ï¸ Changements Critiques

1. **Pattern 5 RÃ©visÃ©** : Lecture complÃ¨te OBLIGATOIRE avant analyses partielles
2. **Pattern 6 AjoutÃ©** : Anti-Angles-Morts comme principe fondamental
3. **HiÃ©rarchie InversÃ©e** : DÃ©lÃ©gation = Pattern #1 prioritaire (80% cas)
4. **Anti-Patterns InversÃ©s** : Lecture complÃ¨te = âœ… BON, Lecture partielle = âŒ MAUVAIS
5. **MÃ©triques Enrichies** : CoÃ»t angles morts inclus dans calculs ROI

### ğŸ›¡ï¸ RÃ¨gle d'Or Universelle

```
AVANT toute dÃ©cision ou modification :
â†’ Contexte COMPLET = OBLIGATOIRE
â†’ Fichier = UnitÃ© atomique (lire EN ENTIER)
â†’ Angles morts = Ennemi silencieux Ã  prÃ©venir
```

### âœ… RÃ©sultat Attendu

- **QualitÃ©** : +85% modifications correctes 1er coup
- **FiabilitÃ©** : -90% bugs subtils par prÃ©vention angles morts
- **ROI** : 300%+ via rÃ©duction re-travail
- **Confiance** : DÃ©cisions Ã©clairÃ©es sur contexte complet

Les patterns prÃ©sentÃ©s permettent de maintenir **qualitÃ© ET performance** tout en respectant les limites des modÃ¨les de langage, en privilÃ©giant approches **sÃ»res** (dÃ©lÃ©gation, lecture complÃ¨te) sur approches **risquÃ©es** (lecture partielle, recherche sans contexte).