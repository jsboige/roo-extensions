{
  "provider": "zai",
  "model": "opus",
  "env": {
    "ANTHROPIC_AUTH_TOKEN": "YOUR_ZAI_API_KEY_HERE",
    "ANTHROPIC_BASE_URL": "https://api.z.ai/api/anthropic",
    "ANTHROPIC_DEFAULT_HAIKU_MODEL": "glm-4.5-air",
    "ANTHROPIC_DEFAULT_SONNET_MODEL": "glm-4.7",
    "ANTHROPIC_DEFAULT_OPUS_MODEL": "glm-5",
    "API_TIMEOUT_MS": "3000000"
  },
  "description": "z.ai GLM models via Anthropic-compatible API",
  "modelMapping": {
    "opus": "GLM-5",
    "sonnet": "GLM-4.7",
    "haiku": "GLM-4.5-Air"
  },
  "defaultModelDetails": {
    "name": "GLM-5",
    "family": "opus",
    "description": "Latest flagship model (2026-02-12), near Opus 4.6 level"
  },
  "notes": [
    "z.ai provides GLM models through an Anthropic-compatible API",
    "GLM-5 (2026-02-12) is the new flagship model, near Opus 4.6 level",
    "GLM-4.7 is mapped to sonnet for balanced performance",
    "GLM-4.5-Air is used for haiku (faster, lighter model)",
    "API timeout increased to 3000s for larger requests",
    "Model mappings set via ANTHROPIC_DEFAULT_*_MODEL environment variables per z.ai documentation"
  ],
  "warnings": {
    "contextTruncation": {
      "detected": "2026-02-19",
      "source": "https://www.reddit.com/r/ZaiGLM/comments/1r675om/inputcontext_truncated_to_137k_tokens/",
      "issue": "z.ai silently truncates context to 137k tokens (vs advertised ~200k+)",
      "impact": "Initial messages get dropped when context fills - can lose system prompts or early conversation",
      "recommendation": "Keep context under 130k tokens, consolidate knowledge before reaching limit"
    }
  }
}
