/**
 * Script de Tests Phase 2 - Tests de Charge Progressifs Indexer-Qdrant
 * Tests 2.1 √† 2.4 du plan de tests
 * 
 * Protocole Go/No-Go strict entre chaque batch
 */

// Charger les variables d'environnement
require('dotenv').config({ path: './mcps/internal/servers/roo-state-manager/.env' });

const https = require('https');
const crypto = require('crypto');
const fetch = (...args) => import('node-fetch').then(({ default: fetch }) => fetch(...args));

// Configuration depuis .env
const QDRANT_URL = process.env.QDRANT_URL || 'https://qdrant.myia.io';
const QDRANT_API_KEY = process.env.QDRANT_API_KEY || '';
const QDRANT_COLLECTION = process.env.QDRANT_COLLECTION_NAME || 'roo_tasks_semantic_index';
const OPENAI_API_KEY = process.env.OPENAI_API_KEY || '';

// Configuration des tests
const RATE_LIMIT_DELAY_MS = 100; // 100ms entre chaque requ√™te
const BATCH_SIZES = [100, 500, 1000]; // 5000 optionnel
const EMBEDDING_BATCH_SIZE = 100; // Batch pour OpenAI pour √©viter rate limit

// Crit√®res Go/No-Go
const CRITERIA = {
  successRate: {
    batch100: 100,     // 100% requis pour baseline
    batch500: 99,      // ‚â•99%
    batch1000: 98,     // ‚â•98%
    batch5000: 90      // ‚â•90%
  },
  avgTimePerDoc: {
    batch100: 2000,    // <2000ms
    batch500: 4000,    // ‚â§2√ó baseline
    batch1000: 3000,   // <3000ms
    batch5000: 5000    // <5000ms
  },
  p95Latency: {
    batch100: 3000,    // <3000ms
    batch500: 5000,    // <5000ms
    batch1000: 200,    // <200ms (objectif prod)
    batch5000: 1000    // <1000ms
  },
  throughput: {
    batch100: 0.5,     // ‚â•0.5 docs/s
    batch500: 0.3,     // ‚â•0.3 docs/s
    batch1000: 0.2,    // ‚â•0.2 docs/s
    batch5000: 0.1     // ‚â•0.1 docs/s
  }
};

// √âtat global
const testState = {
  phase2Results: {},
  allGeneratedTasks: [],
  startTime: Date.now()
};

/**
 * Affichage de la configuration
 */
function printConfiguration() {
  console.log('\nüîß Configuration charg√©e:');
  console.log(`   QDRANT_URL: ${QDRANT_URL}`);
  console.log(`   QDRANT_API_KEY: ${QDRANT_API_KEY ? 'sk-...' + QDRANT_API_KEY.slice(-4) : '‚ùå NON CHARG√âE'}`);
  console.log(`   QDRANT_COLLECTION: ${QDRANT_COLLECTION}`);
  console.log(`   OPENAI_API_KEY: ${OPENAI_API_KEY ? 'sk-...' + OPENAI_API_KEY.slice(-4) : '‚ùå NON CHARG√âE'}`);
  console.log(`   RATE_LIMIT_DELAY: ${RATE_LIMIT_DELAY_MS}ms`);
}

/**
 * G√©n√©rateur de contenu vari√© pour les t√¢ches de test
 */
const contentTemplates = {
  short: [
    "Impl√©menter validation formulaire contact",
    "Corriger bug affichage mobile",
    "Ajouter bouton export CSV",
    "Optimiser requ√™te SQL lente",
    "Mettre √† jour d√©pendances npm",
    "Cr√©er endpoint API status",
    "Ajouter tests unitaires utils",
    "Documenter API REST",
    "Configurer CI/CD pipeline",
    "Corriger fuite m√©moire cache"
  ],
  medium: [
    "Cr√©er une API REST pour la gestion des utilisateurs avec authentification JWT, endpoints CRUD complets, validation des donn√©es d'entr√©e, gestion des erreurs appropri√©e et documentation Swagger.",
    "D√©velopper un syst√®me de notifications en temps r√©el utilisant WebSocket avec reconnexion automatique, file d'attente de messages persistante, et interface utilisateur r√©active avec Toast.",
    "Impl√©menter un module de recherche avanc√©e avec filtres multiples, pagination optimis√©e, tri dynamique, mise en cache intelligente et suggestions en temps r√©el.",
    "Concevoir et d√©velopper un tableau de bord analytics avec graphiques interactifs D3.js, export PDF/Excel, filtres temporels et comparaisons p√©riode sur p√©riode.",
    "Cr√©er un syst√®me de gestion des permissions granulaire avec r√¥les hi√©rarchiques, h√©ritage de permissions, audit trail complet et interface d'administration intuitive.",
    "D√©velopper un module d'import/export de donn√©es supportant CSV, JSON, XML avec validation, transformation, mapping de colonnes et gestion des erreurs d√©taill√©e.",
    "Impl√©menter un syst√®me de cache multi-niveaux avec Redis, strat√©gies d'invalidation intelligentes, monitoring des hit rates et fallback gracieux.",
    "Cr√©er une architecture microservices avec service discovery, load balancing, circuit breaker, distributed tracing et centralized logging.",
    "D√©velopper un syst√®me de versioning d'API avec d√©pr√©ciation progressive, migration automatique des clients, documentation des breaking changes.",
    "Impl√©menter un workflow d'approbation multi-√©tapes avec notifications, historique complet, d√©l√©gation de pouvoir et int√©gration Slack/Teams."
  ],
  long: [
    "D√©velopper une plateforme compl√®te de gestion de projets agiles incluant : tableau Kanban interactif avec drag & drop, sprint planning avec estimation en story points, burndown charts automatiques, gestion des backlogs produit et sprint, syst√®me de notifications configurable par utilisateur, int√©gration Git pour le suivi des commits, rapports de v√©locit√© d'√©quipe, gestion des d√©pendances entre t√¢ches, templates de projets r√©utilisables, et API compl√®te pour int√©grations tierces. Technologies requises : React pour le frontend, Node.js/Express pour le backend, PostgreSQL pour la persistance, Redis pour le cache, Socket.io pour le temps r√©el, JWT pour l'authentification, et Docker pour le d√©ploiement. Contraintes : architecture scalable pour 1000+ utilisateurs simultan√©s, temps de r√©ponse < 200ms, disponibilit√© 99.9%, RGPD compliant.",
    "Concevoir et impl√©menter un syst√®me de recommandation intelligent bas√© sur le machine learning pour une plateforme e-commerce. Le syst√®me doit analyser : historique d'achat des utilisateurs, comportement de navigation (pages vues, temps pass√©, rebonds), donn√©es d√©mographiques, tendances saisonni√®res, stock disponible, et marge produit. Algorithmes √† impl√©menter : collaborative filtering (user-based et item-based), content-based filtering avec NLP sur descriptions produits, hybrid approach combinant les deux, et reinforcement learning pour optimisation continue. Infrastructure : pipeline de donn√©es temps r√©el avec Kafka, feature store avec Redis, mod√®les ML avec TensorFlow Serving, A/B testing framework int√©gr√©. M√©triques √† optimiser : conversion rate, panier moyen, customer lifetime value. Performance requise : pr√©dictions en < 50ms, retraining automatique quotidien, explainability des recommandations.",
    "Cr√©er une infrastructure compl√®te de CI/CD pour une application microservices moderne incluant : pipeline multi-stages (build, test, security scan, deploy), orchestration des d√©ploiements blue-green et canary, rollback automatique sur √©chec des health checks, gestion des secrets avec Vault, scanning de vuln√©rabilit√©s avec Snyk, tests de charge automatis√©s avec k6, monitoring avec Prometheus/Grafana, alerting intelligent avec PagerDuty, et documentation auto-g√©n√©r√©e. Technologies : GitLab CI, Kubernetes, Helm charts, Terraform pour l'IaC, ArgoCD pour GitOps, service mesh avec Istio. Exigences : zero-downtime deployments, compliance SOC2/ISO27001, audit trail complet, disaster recovery avec RPO < 1h, RTO < 30min.",
    "D√©velopper un syst√®me d'authentification et d'autorisation enterprise-grade supportant : SSO avec SAML 2.0 et OpenID Connect, authentification multi-facteur (TOTP, SMS, biom√©trique, hardware tokens), gestion des sessions avec refresh tokens et rotation automatique, politique de mots de passe configurable avec historique, lockout apr√®s tentatives √©chou√©es, r√©initialisation s√©curis√©e par email/SMS, int√©gration Active Directory et LDAP, API de provisioning automatique des comptes, audit logging d√©taill√© de toutes les actions, conformit√© RGPD avec consentement explicite et droit √† l'oubli. Architecture : service ind√©pendant scalable horizontalement, base de donn√©es chiffr√©e au repos et en transit, rate limiting par IP et par utilisateur, protection contre brute force et credential stuffing. Certifications requises : OAuth 2.0, OpenID Connect, PCI-DSS Level 1.",
    "Impl√©menter une solution compl√®te de data pipeline ETL pour analytics big data : ingestion de donn√©es multi-sources (APIs REST, fichiers CSV/JSON/Parquet, bases SQL/NoSQL, streaming Kafka), transformation avec validation de qualit√© et nettoyage, enrichissement avec donn√©es externes, agr√©gations pr√©-calcul√©es pour performance, stockage dans data warehouse columnar (Clickhouse ou Snowflake), et serving via API GraphQL. Processing : batch quotidien et streaming temps r√©el avec Apache Spark, orchestration avec Airflow, monitoring de la qualit√© des donn√©es avec Great Expectations, gestion des erreurs avec retry logic et dead letter queue. Scalabilit√© : support de p√©taoctets de donn√©es, processing parall√®le massif, partitionnement intelligent par date/r√©gion. Gouvernance : data lineage tra√ßable, metadata management, access control granulaire, data masking pour PII."
  ]
};

const statusOptions = ['pending', 'in_progress', 'completed', 'blocked', 'cancelled'];
const priorityOptions = ['low', 'medium', 'high', 'urgent'];

/**
 * G√©n√®re une t√¢che de test avec contenu vari√©
 */
function generateTestTask(batchNum, docNum, type = 'medium') {
  const templates = contentTemplates[type];
  const content = templates[Math.floor(Math.random() * templates.length)];
  const status = statusOptions[Math.floor(Math.random() * statusOptions.length)];
  const priority = priorityOptions[Math.floor(Math.random() * priorityOptions.length)];
  
  return {
    id: `test-phase2-batch${batchNum}-doc${docNum}`,
    description: content,
    status: status,
    priority: priority,
    created_at: new Date(Date.now() - Math.random() * 30 * 24 * 60 * 60 * 1000).toISOString(),
    metadata: {
      test_phase: 2,
      batch: batchNum,
      doc_type: type,
      doc_number: docNum
    }
  };
}

/**
 * G√©n√®re un batch de t√¢ches vari√©es
 */
function generateBatch(batchNum, size) {
  const tasks = [];
  const distribution = {
    short: Math.floor(size * 0.25),
    medium: Math.floor(size * 0.50),
    long: Math.ceil(size * 0.25)
  };
  
  let docNum = 1;
  
  // T√¢ches courtes (25%)
  for (let i = 0; i < distribution.short; i++) {
    tasks.push(generateTestTask(batchNum, docNum++, 'short'));
  }
  
  // T√¢ches moyennes (50%)
  for (let i = 0; i < distribution.medium; i++) {
    tasks.push(generateTestTask(batchNum, docNum++, 'medium'));
  }
  
  // T√¢ches longues (25%)
  for (let i = 0; i < distribution.long; i++) {
    tasks.push(generateTestTask(batchNum, docNum++, 'long'));
  }
  
  return tasks;
}

/**
 * Cr√©e un embedding via OpenAI
 */
async function createEmbedding(text) {
  try {
    const response = await fetch('https://api.openai.com/v1/embeddings', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'text-embedding-3-small',
        input: text
      })
    });
    
    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.status}`);
    }
    
    const data = await response.json();
    return {
      embedding: data.data[0].embedding,
      tokens: data.usage.total_tokens
    };
  } catch (error) {
    console.error(`‚ùå Erreur cr√©ation embedding: ${error.message}`);
    return null;
  }
}

/**
 * Ins√®re un point dans Qdrant
 */
async function upsertPoint(task, embedding) {
  try {
    const pointId = crypto.randomUUID();
    const point = {
      id: pointId,
      vector: embedding,
      payload: {
        task_id: task.id,
        chunk_id: `chunk-${pointId}`,
        content: task.description,
        content_summary: task.description.substring(0, 100),
        chunk_type: task.metadata.doc_type,
        status: task.status,
        priority: task.priority,
        timestamp: task.created_at,
        workspace: '/test/phase2',
        indexed: true,
        test_phase: task.metadata.test_phase,
        batch: task.metadata.batch
      }
    };
    
    const response = await fetch(`${QDRANT_URL}/collections/${QDRANT_COLLECTION}/points`, {
      method: 'PUT',
      headers: {
        'api-key': QDRANT_API_KEY,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        points: [point]
      })
    });
    
    if (!response.ok) {
      throw new Error(`Qdrant upsert error: ${response.status}`);
    }
    
    return { success: true, pointId };
  } catch (error) {
    return { success: false, error: error.message };
  }
}

/**
 * Calcule les statistiques d'un tableau de valeurs
 */
function calculateStats(values) {
  if (values.length === 0) return { min: 0, max: 0, avg: 0, p50: 0, p95: 0, p99: 0 };
  
  const sorted = [...values].sort((a, b) => a - b);
  const sum = values.reduce((acc, v) => acc + v, 0);
  
  return {
    min: Math.round(sorted[0]),
    max: Math.round(sorted[sorted.length - 1]),
    avg: Math.round(sum / values.length),
    p50: Math.round(sorted[Math.floor(sorted.length * 0.5)]),
    p95: Math.round(sorted[Math.floor(sorted.length * 0.95)]),
    p99: Math.round(sorted[Math.floor(sorted.length * 0.99)])
  };
}

/**
 * Ex√©cute un batch de tests de charge
 */
async function runBatch(batchNum, size) {
  console.log(`\n${'‚ïê'.repeat(60)}`);
  console.log(`   TEST 2.${batchNum} : BATCH ${size} DOCUMENTS`);
  console.log(`${'‚ïê'.repeat(60)}`);
  
  const startTime = Date.now();
  const metrics = {
    batchNum,
    size,
    startTime: new Date().toISOString(),
    tasksGenerated: 0,
    embeddingsCreated: 0,
    pointsIndexed: 0,
    errors: [],
    latencies: [],
    embeddingTokens: 0,
    status: 'pending'
  };
  
  // √âtape 1 : G√©n√©ration des t√¢ches
  console.log(`\nüìã G√©n√©ration de ${size} t√¢ches de test...`);
  const tasks = generateBatch(batchNum, size);
  metrics.tasksGenerated = tasks.length;
  testState.allGeneratedTasks.push(...tasks);
  console.log(`‚úÖ ${tasks.length} t√¢ches g√©n√©r√©es`);
  
  // √âtape 2 : Cr√©ation des embeddings par batches
  console.log(`\nüß† Cr√©ation des embeddings (batches de ${EMBEDDING_BATCH_SIZE})...`);
  const taskEmbeddings = [];
  
  for (let i = 0; i < tasks.length; i += EMBEDDING_BATCH_SIZE) {
    const batchTasks = tasks.slice(i, i + EMBEDDING_BATCH_SIZE);
    console.log(`   Processing batch ${Math.floor(i / EMBEDDING_BATCH_SIZE) + 1}/${Math.ceil(tasks.length / EMBEDDING_BATCH_SIZE)} (${batchTasks.length} t√¢ches)...`);
    
    for (const task of batchTasks) {
      const embStart = Date.now();
      const result = await createEmbedding(task.description);
      const embDuration = Date.now() - embStart;
      
      if (result) {
        taskEmbeddings.push({ task, embedding: result.embedding });
        metrics.embeddingsCreated++;
        metrics.embeddingTokens += result.tokens;
        metrics.latencies.push(embDuration);
      } else {
        metrics.errors.push({
          step: 'embedding',
          taskId: task.id,
          error: 'Failed to create embedding'
        });
      }
      
      // Rate limiting
      await new Promise(resolve => setTimeout(resolve, RATE_LIMIT_DELAY_MS));
    }
    
    // Pause suppl√©mentaire entre batches OpenAI
    if (i + EMBEDDING_BATCH_SIZE < tasks.length) {
      await new Promise(resolve => setTimeout(resolve, 500));
    }
  }
  
  console.log(`‚úÖ ${metrics.embeddingsCreated}/${tasks.length} embeddings cr√©√©s`);
  console.log(`   Tokens utilis√©s: ${metrics.embeddingTokens}`);
  
  // √âtape 3 : Indexation dans Qdrant
  console.log(`\nüì• Indexation dans Qdrant...`);
  let progressCounter = 0;
  const progressInterval = Math.max(1, Math.floor(taskEmbeddings.length / 20)); // Afficher 20 updates max
  
  for (const { task, embedding } of taskEmbeddings) {
    const upsertStart = Date.now();
    const result = await upsertPoint(task, embedding);
    const upsertDuration = Date.now() - upsertStart;
    
    if (result.success) {
      metrics.pointsIndexed++;
      metrics.latencies.push(upsertDuration);
    } else {
      metrics.errors.push({
        step: 'upsert',
        taskId: task.id,
        error: result.error
      });
    }
    
    progressCounter++;
    if (progressCounter % progressInterval === 0) {
      const percent = Math.round((progressCounter / taskEmbeddings.length) * 100);
      console.log(`   Progress: ${percent}% (${progressCounter}/${taskEmbeddings.length})`);
    }
    
    // Rate limiting
    await new Promise(resolve => setTimeout(resolve, RATE_LIMIT_DELAY_MS));
  }
  
  const endTime = Date.now();
  const totalDuration = endTime - startTime;
  
  // Calcul des m√©triques finales
  metrics.endTime = new Date().toISOString();
  metrics.totalDuration = totalDuration;
  metrics.avgTimePerDoc = Math.round(totalDuration / size);
  metrics.throughput = (size / (totalDuration / 1000)).toFixed(2);
  metrics.successRate = ((metrics.pointsIndexed / size) * 100).toFixed(2);
  metrics.latencyStats = calculateStats(metrics.latencies);
  
  // D√©terminer le statut
  const batchKey = `batch${size}`;
  const criteria = CRITERIA;
  const successRateOk = parseFloat(metrics.successRate) >= criteria.successRate[batchKey];
  const avgTimeOk = metrics.avgTimePerDoc <= criteria.avgTimePerDoc[batchKey];
  const p95Ok = metrics.latencyStats.p95 <= criteria.p95Latency[batchKey];
  const throughputOk = parseFloat(metrics.throughput) >= criteria.throughput[batchKey];
  
  metrics.status = successRateOk && avgTimeOk && p95Ok && throughputOk ? 'success' : 'failed';
  metrics.criteriaCheck = {
    successRate: { value: metrics.successRate, threshold: criteria.successRate[batchKey], pass: successRateOk },
    avgTimePerDoc: { value: metrics.avgTimePerDoc, threshold: criteria.avgTimePerDoc[batchKey], pass: avgTimeOk },
    p95Latency: { value: metrics.latencyStats.p95, threshold: criteria.p95Latency[batchKey], pass: p95Ok },
    throughput: { value: metrics.throughput, threshold: criteria.throughput[batchKey], pass: throughputOk }
  };
  
  console.log(`\n‚úÖ Batch ${size} termin√© en ${(totalDuration / 1000).toFixed(1)}s`);
  console.log(`   Documents index√©s: ${metrics.pointsIndexed}/${size}`);
  console.log(`   Taux de succ√®s: ${metrics.successRate}%`);
  console.log(`   D√©bit: ${metrics.throughput} docs/s`);
  console.log(`   Temps moyen: ${metrics.avgTimePerDoc}ms/doc`);
  console.log(`   Latence P95: ${metrics.latencyStats.p95}ms`);
  
  return metrics;
}

/**
 * V√©rification Go/No-Go
 */
function checkGoNoGo(metrics, batchSize) {
  console.log(`\n${'‚ïê'.repeat(60)}`);
  console.log(`   V√âRIFICATION GO/NO-GO - BATCH ${batchSize}`);
  console.log(`${'‚ïê'.repeat(60)}`);
  
  const criteria = metrics.criteriaCheck;
  let allPass = true;
  
  console.log(`\nüìä Crit√®res de validation:`);
  console.log(`   ‚úì Taux de succ√®s: ${criteria.successRate.value}% (seuil: ‚â•${criteria.successRate.threshold}%) ${criteria.successRate.pass ? '‚úÖ' : '‚ùå'}`);
  console.log(`   ‚úì Temps moyen/doc: ${criteria.avgTimePerDoc.value}ms (seuil: <${criteria.avgTimePerDoc.threshold}ms) ${criteria.avgTimePerDoc.pass ? '‚úÖ' : '‚ùå'}`);
  console.log(`   ‚úì Latence P95: ${criteria.p95Latency.value}ms (seuil: <${criteria.p95Latency.threshold}ms) ${criteria.p95Latency.pass ? '‚úÖ' : '‚ùå'}`);
  console.log(`   ‚úì D√©bit: ${criteria.throughput.value} docs/s (seuil: ‚â•${criteria.throughput.threshold} docs/s) ${criteria.throughput.pass ? '‚úÖ' : '‚ùå'}`);
  
  allPass = criteria.successRate.pass && criteria.avgTimePerDoc.pass && 
            criteria.p95Latency.pass && criteria.throughput.pass;
  
  const decision = allPass ? 'GO' : 'NO-GO';
  const emoji = allPass ? '‚úÖ' : '‚ùå';
  
  console.log(`\n${emoji} D√âCISION: ${decision} pour le batch suivant`);
  
  if (!allPass) {
    console.log(`\n‚ö†Ô∏è ARR√äT DES TESTS - Crit√®res non atteints`);
    console.log(`   Documenter les r√©sultats et analyser les √©checs avant de continuer.`);
  }
  
  return allPass;
}

/**
 * Effectue quelques recherches s√©mantiques de validation
 */
async function validateSemanticSearch(sampleSize = 5) {
  console.log(`\nüîç Validation recherche s√©mantique (${sampleSize} requ√™tes)...`);
  
  const queries = [
    "authentification utilisateur JWT",
    "optimisation performance base de donn√©es",
    "syst√®me de notifications temps r√©el",
    "gestion des erreurs API REST",
    "monitoring et logging applicatif"
  ];
  
  let successCount = 0;
  
  for (const query of queries.slice(0, sampleSize)) {
    try {
      const embResult = await createEmbedding(query);
      if (!embResult) continue;
      
      const response = await fetch(`${QDRANT_URL}/collections/${QDRANT_COLLECTION}/points/search`, {
        method: 'POST',
        headers: {
          'api-key': QDRANT_API_KEY,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          vector: embResult.embedding,
          limit: 3,
          with_payload: true
        })
      });
      
      if (response.ok) {
        const data = await response.json();
        const resultsFound = data.result?.length || 0;
        console.log(`   ‚úÖ "${query}": ${resultsFound} r√©sultats`);
        successCount++;
      }
      
      await new Promise(resolve => setTimeout(resolve, RATE_LIMIT_DELAY_MS));
    } catch (error) {
      console.log(`   ‚ùå "${query}": ${error.message}`);
    }
  }
  
  console.log(`\n   R√©sum√©: ${successCount}/${sampleSize} recherches r√©ussies`);
  return successCount === sampleSize;
}

/**
 * G√©n√®re le rapport final
 */
async function generateFinalReport() {
  const totalDuration = Date.now() - testState.startTime;
  const batches = Object.values(testState.phase2Results);
  
  const report = {
    timestamp: new Date().toISOString(),
    duration_ms: totalDuration,
    duration_formatted: `${Math.floor(totalDuration / 60000)}min ${Math.round((totalDuration % 60000) / 1000)}s`,
    batches_completed: batches.length,
    total_tasks_generated: testState.allGeneratedTasks.length,
    total_points_indexed: batches.reduce((sum, b) => sum + b.pointsIndexed, 0),
    global_success_rate: (batches.reduce((sum, b) => sum + parseFloat(b.successRate), 0) / batches.length).toFixed(2),
    verdict: batches.every(b => b.status === 'success') ? 'GO Phase 3' : 'NO-GO Phase 3',
    batches: batches,
    recommendations: []
  };
  
  // G√©n√©rer recommandations
  if (parseFloat(report.global_success_rate) < 99) {
    report.recommendations.push("‚ö†Ô∏è Taux de succ√®s global < 99% - Analyser les erreurs d'indexation");
  }
  
  const avgThroughput = batches.reduce((sum, b) => sum + parseFloat(b.throughput), 0) / batches.length;
  if (avgThroughput < 0.3) {
    report.recommendations.push("‚ö†Ô∏è D√©bit moyen < 0.3 docs/s - Optimiser le rate limiting ou parall√©liser");
  }
  
  const hasHighLatency = batches.some(b => b.latencyStats.p95 > 1000);
  if (hasHighLatency) {
    report.recommendations.push("‚ö†Ô∏è Latence P95 √©lev√©e d√©tect√©e - V√©rifier performances Qdrant et r√©seau");
  }
  
  if (report.recommendations.length === 0) {
    report.recommendations.push("‚úÖ Tous les crit√®res sont satisfaisants - Pr√™t pour production");
  }
  
  return report;
}

/**
 * Sauvegarde le rapport
 */
async function saveReport(report) {
  const fs = require('fs');
  const path = require('path');
  
  const timestamp = new Date().toISOString().replace(/[:.]/g, '-').substring(0, 16);
  const reportDir = path.join(__dirname, '../docs/testing/reports');
  const reportPath = path.join(reportDir, `phase2-charge-${timestamp}.md`);
  
  // Cr√©er le r√©pertoire si n√©cessaire
  if (!fs.existsSync(reportDir)) {
    fs.mkdirSync(reportDir, { recursive: true });
  }
  
  // G√©n√©rer le contenu Markdown
  let markdown = `# Rapport Phase 2 : Tests de Charge Indexer-Qdrant\n\n`;
  markdown += `**Date** : ${report.timestamp}\n`;
  markdown += `**Dur√©e** : ${report.duration_formatted}\n\n`;
  markdown += `## R√©sum√© Ex√©cutif\n\n`;
  markdown += `- **Tests r√©alis√©s** : ${report.batches_completed}/4 batches\n`;
  markdown += `- **Documents index√©s** : ${report.total_points_indexed}/${report.total_tasks_generated} total\n`;
  markdown += `- **Taux de succ√®s global** : ${report.global_success_rate}%\n`;
  markdown += `- **D√©cision** : **${report.verdict}**\n\n`;
  
  markdown += `## R√©sultats par Batch\n\n`;
  
  for (const batch of report.batches) {
    const icon = batch.status === 'success' ? '‚úÖ' : '‚ùå';
    markdown += `### Test 2.${batch.batchNum} : Batch ${batch.size} ${icon}\n\n`;
    markdown += `- **Statut** : ${batch.status.toUpperCase()}\n`;
    markdown += `- **Documents** : ${batch.pointsIndexed}/${batch.size}\n`;
    markdown += `- **Temps total** : ${(batch.totalDuration / 1000).toFixed(1)}s\n`;
    markdown += `- **Temps moyen** : ${batch.avgTimePerDoc}ms/doc\n`;
    markdown += `- **D√©bit** : ${batch.throughput} docs/s\n`;
    markdown += `- **Latence P95** : ${batch.latencyStats.p95}ms\n`;
    markdown += `- **Taux de succ√®s** : ${batch.successRate}%\n`;
    markdown += `- **Erreurs** : ${batch.errors.length}\n\n`;
    
    markdown += `**Crit√®res de validation** :\n`;
    for (const [key, criterion] of Object.entries(batch.criteriaCheck)) {
      const icon = criterion.pass ? '‚úÖ' : '‚ùå';
      markdown += `- ${icon} ${key}: ${criterion.value} (seuil: ${criterion.threshold})\n`;
    }
    markdown += `\n`;
  }
  
  markdown += `## M√©triques Comparatives\n\n`;
  markdown += `| Batch | Taille | Dur√©e (s) | D√©bit (docs/s) | Latence P95 (ms) | Succ√®s (%) |\n`;
  markdown += `|-------|--------|-----------|----------------|------------------|------------|\n`;
  for (const batch of report.batches) {
    markdown += `| ${batch.batchNum} | ${batch.size} | ${(batch.totalDuration / 1000).toFixed(1)} | ${batch.throughput} | ${batch.latencyStats.p95} | ${batch.successRate} |\n`;
  }
  markdown += `\n`;
  
  markdown += `## Recommandations\n\n`;
  for (const rec of report.recommendations) {
    markdown += `${rec}\n\n`;
  }
  
  markdown += `## D√©tails Techniques\n\n`;
  markdown += `- **QDRANT_URL** : ${QDRANT_URL}\n`;
  markdown += `- **QDRANT_COLLECTION** : ${QDRANT_COLLECTION}\n`;
  markdown += `- **Rate Limiter** : ${RATE_LIMIT_DELAY_MS}ms entre requ√™tes\n`;
  markdown += `- **Embedding Model** : text-embedding-3-small (1536 dimensions)\n`;
  markdown += `- **Total Tokens OpenAI** : ${report.batches.reduce((sum, b) => sum + b.embeddingTokens, 0)}\n\n`;
  
  markdown += `---\n\n`;
  markdown += `*Rapport g√©n√©r√© automatiquement par tests/indexer-phase2-load-tests.cjs*\n`;
  
  fs.writeFileSync(reportPath, markdown, 'utf-8');
  
  console.log(`\nüìÑ Rapport sauvegard√©: ${reportPath}`);
  return reportPath;
}

/**
 * Ex√©cution principale
 */
async function main() {
  console.log('‚ïê'.repeat(60));
  console.log('   TESTS PHASE 2 - Tests de Charge Progressifs');
  console.log('‚ïê'.repeat(60));
  console.log(`Timestamp: ${new Date().toISOString()}`);
  
  printConfiguration();
  
  // Validation environnement
  if (!OPENAI_API_KEY || !QDRANT_API_KEY) {
    console.error('\n‚ùå ERREUR: Variables d\'environnement manquantes');
    process.exit(1);
  }
  
  // Ex√©cution des batches
  for (const size of BATCH_SIZES) {
    const metrics = await runBatch(BATCH_SIZES.indexOf(size) + 1, size);
    testState.phase2Results[`batch${size}`] = metrics;
    
    // Go/No-Go check
    const canContinue = checkGoNoGo(metrics, size);
    
    if (!canContinue) {
      console.log(`\n‚õî Arr√™t des tests apr√®s batch ${size}`);
      break;
    }
    
    // Pause entre batches
    if (size < BATCH_SIZES[BATCH_SIZES.length - 1]) {
      console.log(`\n‚è∏Ô∏è Pause de 10s avant le batch suivant...`);
      await new Promise(resolve => setTimeout(resolve, 10000));
    }
  }
  
  // Validation recherche s√©mantique
  await validateSemanticSearch();
  
  // G√©n√©ration rapport final
  console.log(`\n${'‚ïê'.repeat(60)}`);
  console.log('   G√âN√âRATION DU RAPPORT FINAL');
  console.log('‚ïê'.repeat(60));
  
  const report = await generateFinalReport();
  const reportPath = await saveReport(report);
  
  // Affichage r√©sum√©
  console.log(`\n${'‚ïê'.repeat(60)}`);
  console.log('   R√âSUM√â FINAL PHASE 2');
  console.log(`${'‚ïê'.repeat(60)}\n`);
  console.log(`Batches compl√©t√©s: ${report.batches_completed}`);
  console.log(`Documents index√©s: ${report.total_points_indexed}/${report.total_tasks_generated}`);
  console.log(`Taux de succ√®s global: ${report.global_success_rate}%`);
  console.log(`Dur√©e totale: ${report.duration_formatted}`);
  console.log(`\nüéØ VERDICT: ${report.verdict}`);
  
  console.log(`\n‚úÖ Tests Phase 2 termin√©s avec succ√®s`);
}

// Ex√©cution
main().catch(error => {
  console.error('\nüí• ERREUR FATALE:', error);
  process.exit(1);
});